{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PV.ipynb","provenance":[],"mount_file_id":"1b8c2sLBalAYZQJFuaJgf1XOFpruYTz0t","authorship_tag":"ABX9TyPSsY6jZov0xACtTkOPsNG0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YL4uELpshzdD","colab_type":"code","outputId":"caa9762e-fde7-4bdb-e8a3-5ae8b8b68f40","executionInfo":{"status":"ok","timestamp":1580297660490,"user_tz":-60,"elapsed":751062,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":918}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","    TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","    When encoder is traine completely, merged model exhibits much better\n","    performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","    encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","    decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","%tensorflow_version 2.x\n","import os\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","\n","import tensorflow as tf\n","\n","import matplotlib\n","\n","tf.test.gpu_device_name()\n","\n","with tf.device('/device:GPU:0'):\n","  # Different configurations for different computation structures\n","  # gpu = input('On Server?(y/n) : ')\n","  gpu = 'n'\n","  if(gpu == 'y'):\n","      # Ask for the parameters\n","      encoderEpoch    = 10\n","      decoderEpoch    = 5\n","      numNeuron       = 128\n","      batchSize       = 1\n","\n","      forecastHorizon = input('T - M         : ')\n","      directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = input('Data size     : ')\n","      M               = input('M             : ')\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","      stateful = True\n","\n","\n","      # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","  else:\n","      # Ask for the parameters\n","      encoderEpoch    = 100\n","      decoderEpoch    = 300\n","      numNeuron       = 512\n","      batchSize       = 1\n","\n","      forecastHorizon = '24'\n","      directoryName   = '/content/drive/My Drive/Colab Notebooks/pv' # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = '1440'\n","      M               = '24'\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","\n","\n","      stateful = False\n","\n","\n","\n","      os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","  import matplotlib.pyplot as plt\n","\n","\n","\n","\n","  #################### Code starts\n","  # Parameters for LSTM input (we need to know the input shape)\n","  # These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","  num_months = 0\n","  num_days   = 0\n","  num_hours  = 0\n","\n","  def plot(figNum, originalData, lstmOuts, customTag=''):\n","      # lstmOuts -> predictions inverse transformed\n","      # originalData -> data read from file\n","\n","      # forecastLong : forecast with given (T - M)\n","      forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      listStart = -50\n","      listEnd   = -2\n","      plt.plot(originalData[listStart:listEnd])\n","      plt.plot(forecastLong[listStart:listEnd])\n","      plt.plot(persis[listStart+1:listEnd+1])\n","      plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","      title = ''\n","      title = title + customTag + '\\n'\n","      title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","      title = title + ('dataSize = ' + str(dataSize) + ', ')\n","      title = title + ('M = ' + str(M) + ', ')\n","      title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","      title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","      # title = title + ('loss = ' + str(loss) + ', ')\n","      # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","      title = title + ('(T - M) = ' + str(T-M) + ', ')\n","      title = title + ('currentEpoch = ' + str(figNum) + '')\n","      plt.title(title, fontsize=6)\n","\n","      plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","      plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","      plt.savefig(directoryName + '/' + customTag + str(figNum) + '.png', bbox_inches=\"tight\")\n","      plt.cla()\n","\n","\n","  def noneNum(allorders):\n","      # Replace None's in a python list with 0's and return the output\n","      allorders = list(allorders)\n","      output = [i if i is not None else 0 for i in allorders]\n","\n","      return output\n","\n","  def calculateError(originalData, lstmOuts):\n","      forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      # Calculate error\n","      endIndex = -(T - M) + 1\n","      if endIndex == 0:\n","          endIndex = originalData.shape[0]\n","\n","      text = ''\n","\n","      maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MAE my model     : \" + str(maemym)\n","      text = text + '\\n'\n","      text = text + \"MAE persistence  : \" + str(maepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MSE my model     : \" + str(msemym)\n","      text = text + '\\n'\n","      text = text + \"MSE persistence  : \" + str(msepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      # # TODO - can be replaced by tensorflow\n","      # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # text = text + \"MAPE my model    : \" + str(mapemym)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","      \n","      # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # with tensorflow.Session() as sess:\n","          # mapemym_val = mapemym.eval()\n","          # mapepers_val = mapepers.eval()\n","      \n","      # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","\n","\n","      return text\n","\n","  #def mean_absolute_percentage_error(y_true, y_pred):\n","  #    # Not implemented in sklearn\n","  #\n","  #    # Not recommended to use since negative values\n","  #    # create negative losses\n","  #\n","  #    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","  def readData(dataType):\n","      if dataType == 'train':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_train.csv',sep=',')\n","      elif dataType == 'valid':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_valid.csv',sep=',')\n","      elif dataType == 'test':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_test.csv',sep=',')\n","\n","      outputDat = np.array(data.total_pv.iloc[:len(data)])\n","      date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","      return outputDat, date_time\n","\n","\n","  # Taken from test3.py\n","  # Taken from test3.py\n","  def prepareDataTraining(Xa, date_time):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","      \n","      dataSize = len(Xa)\n","\n","      scaler = MinMaxScaler(feature_range =(-1,1))\n","      X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","      \n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","      onehot_day_of_week   = OneHotEncoder(sparse=False)\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","      print(type(all_hours))\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","  # Taken from test3.py\n","  def prepareDataTesting(Xa, date_time):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","\n","      dataSize = len(Xa)\n","      scaler = MinMaxScaler(feature_range =(-1,1))\n","      X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","      onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","  global forecasts\n","\n","  ############# Run code\n","  trainData, train_date_time = readData('train')\n","  testData,  test_date_time  = readData('valid')\n","  validData,  valid_date_time  = readData('test')\n","\n","  trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","  testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time, trainScaler)\n","  validScaler,  XvalidEncoder,  YvalidEncoder,  XvalidDecoder,  YvalidDecoder  = prepareDataTesting(validData , valid_date_time, trainScaler)\n","\n","\n","\n","  ### Tensorboard modelling\n","  logdirEnc = \"pv_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  logdirDec = \"/content/drive/My Drive/Colab Notebooks/pv/decoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","  tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","  ### Create Encoder model\n","\n","  # Model layers\n","  num_features_encoder = 1 + (num_months + num_days + num_hours)\n","  inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","  encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful, activation = \"relu\")\n","  # encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  #encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  encoderDense = TimeDistributed(Dense(1))\n","\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, _  = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","  # encoderLSTM2Outputs, _, _      = encoderLSTM2(encoderLSTMOutputs)\n","  #encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\n","  encoderOutputs           = encoderDense(encoderLSTMOutputs)\n","\n","\n","  encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","  encoderModel.compile(optimizer='adam', loss='mse')\n","\n","  encoderFilepath=\"/content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n","  encoderCheckpoint = ModelCheckpoint(encoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","\n","  print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[encoderCheckpoint,tensorboardCallbackEncoder])\n","  # encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/encoder_pv_e100.hdf5')\n","\n","\n","  ### Create merged (Encoder + Decoder) model\n","\n","  # Model layers\n","  num_features_decoder   = (num_months + num_days + num_hours)\n","  inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","  decoderBroadcasted     = Lambda(lambda x: broadcast_to(x, [batchSize, T-M, numNeuron]))\n","  decoderConcatenated    = Lambda(lambda x: concat(x, axis=2))\n","  decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","  decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.4,activation=\"relu\")\n","  # decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.4,activation=\"relu\")\n","  #decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","  decoderDense           = TimeDistributed(Dense(1))\n","  layerV                 = Dense(numNeuron)\n","  layerVActivation       = Activation('relu')\n","  layerV_prime           = Dense(numNeuron)\n","  layerV_primeActivation = Activation('relu')\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","  # Map encoder hidden state final to some vector(simple layer)\n","  c = layerVActivation(layerV(encoder_h))\n","  # Remap the c vector to h_prime\n","  h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","  # Use h_prime as input to other layers\n","  c_broadcasted              = decoderBroadcasted(c)\n","  decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","  # decoderStateZeros : just zeros to feed to the internal state\n","  decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","  decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","  # decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","  #decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","  decoderOutputs     = decoderDense(decoderLSTMOutputs)\n","\n","\n","  mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","  mergedModel.compile(optimizer='adam', loss='mse')\n","  print('Training merged(encoder + decoder) model...');\n","\n","  outputs = list()\n","\n","  \n","  \n","\n","  for i in range(decoderEpoch):\n","      decoderFilepath=\"/content/drive/My Drive/Colab Notebooks/pv/models/decoder-weights-improvement-\"+str(i)+\"-{val_loss:.2f}.hdf5\"\n","      decoderCheckpoint = ModelCheckpoint(decoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","      mergedModel.fit([XtrainEncoder, XtrainDecoder], [YtrainDecoder],epochs=1, shuffle=False, validation_data=([XtestEncoder, XtestDecoder], YtestDecoder),callbacks=[decoderCheckpoint,tensorboardCallbackDecoder])\n","\n","\n","      lstmOuts = mergedModel.predict([XtrainEncoder, XtrainDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = trainScaler.inverse_transform(lstmOuts)\n","      plot(i, trainData, lstmOuts, 'train')\n","      error_train = calculateError(trainData, lstmOuts)\n","\n","      lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = testScaler.inverse_transform(lstmOuts)\n","      plot(i, testData, lstmOuts, 'test')\n","      error_test = calculateError(testData, lstmOuts)\n","      outputs.append(lstmOuts[-1])\n","      outputsDF = pd.DataFrame(outputs)\n","      if i%10 == 0:\n","          outputsDF.to_csv(directoryName + '/' + 'output_' + str(i) + '.csv', sep = ',')\n","          outputs = list()\n","      lstmOuts = mergedModel.predict([XvalidEncoder, XvalidDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = validScaler.inverse_transform(lstmOuts)\n","      plotTest(i, validData, lstmOuts, 'test')\n","      \n","      print('train errors: ', error_train)\n","      print('test errors: ',error_test)\n","  \n","  #np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","Training encoder...\n","Train on 1248 samples, validate on 384 samples\n","Epoch 1/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0621\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-01-0.04.hdf5\n","1248/1248 [==============================] - 50s 40ms/sample - loss: 0.0620 - val_loss: 0.0446\n","Epoch 2/100\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0300\n","Epoch 00002: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-02-0.05.hdf5\n","1248/1248 [==============================] - 50s 40ms/sample - loss: 0.0300 - val_loss: 0.0491\n","Epoch 3/100\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0251\n","Epoch 00003: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-03-0.05.hdf5\n","1248/1248 [==============================] - 53s 42ms/sample - loss: 0.0251 - val_loss: 0.0473\n","Epoch 4/100\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0221\n","Epoch 00004: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-04-0.04.hdf5\n","1248/1248 [==============================] - 50s 40ms/sample - loss: 0.0221 - val_loss: 0.0435\n","Epoch 5/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0200\n","Epoch 00005: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-05-0.05.hdf5\n","1248/1248 [==============================] - 50s 40ms/sample - loss: 0.0200 - val_loss: 0.0466\n","Epoch 6/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0165\n","Epoch 00006: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-06-0.05.hdf5\n","1248/1248 [==============================] - 51s 40ms/sample - loss: 0.0165 - val_loss: 0.0510\n","Epoch 7/100\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0129\n","Epoch 00007: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-07-0.07.hdf5\n","1248/1248 [==============================] - 50s 40ms/sample - loss: 0.0129 - val_loss: 0.0670\n","Epoch 8/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0107\n","Epoch 00008: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-08-0.06.hdf5\n","1248/1248 [==============================] - 50s 40ms/sample - loss: 0.0107 - val_loss: 0.0621\n","Epoch 9/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0093\n","Epoch 00009: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-09-0.04.hdf5\n","1248/1248 [==============================] - 49s 40ms/sample - loss: 0.0093 - val_loss: 0.0405\n","Epoch 10/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00010: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-10-0.04.hdf5\n","1248/1248 [==============================] - 49s 40ms/sample - loss: 0.0076 - val_loss: 0.0428\n","Epoch 11/100\n","1246/1248 [============================>.] - ETA: 0s - loss: 0.0069\n","Epoch 00011: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-11-0.04.hdf5\n","1248/1248 [==============================] - 48s 39ms/sample - loss: 0.0069 - val_loss: 0.0417\n","Epoch 12/100\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0063\n","Epoch 00012: saving model to /content/drive/My Drive/Colab Notebooks/pv/models/encoder-weights-improvement-12-0.04.hdf5\n","1248/1248 [==============================] - 51s 41ms/sample - loss: 0.0063 - val_loss: 0.0385\n","Epoch 13/100\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0055"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LK8uxCbyUwPZ","colab_type":"code","outputId":"f0157e52-6c7c-47cc-de1d-f55f59bb6740","executionInfo":{"status":"ok","timestamp":1580906997118,"user_tz":-60,"elapsed":23322,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lDJlkYrhit14","colab_type":"code","outputId":"986985db-bc85-485f-df23-326e31fff1e1","executionInfo":{"status":"error","timestamp":1580898311485,"user_tz":-60,"elapsed":139001,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from google.colab import files\n","!zip -r pv.zip pv\n","files.download(\"pv.zip\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  adding: pv/ (stored 0%)\n","  adding: pv/train24.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-23-0.19.hdf5 (deflated 21%)\n","  adding: pv/decoder-weights-improvement-15-0.21.hdf5 (deflated 20%)\n","  adding: pv/encoder-weights-improvement-97-0.05.hdf5 (deflated 26%)\n","  adding: pv/encoder-weights-improvement-44-0.10.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-92-0.05.hdf5 (deflated 26%)\n","  adding: pv/encoder-weights-improvement-30-0.05.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-66-0.05.hdf5 (deflated 24%)\n","  adding: pv/test14.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-08-0.08.hdf5 (deflated 11%)\n","  adding: pv/test17.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-62-0.05.hdf5 (deflated 23%)\n","  adding: pv/encoder-weights-improvement-75-0.04.hdf5 (deflated 24%)\n","  adding: pv/test27.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-100-0.05.hdf5 (deflated 26%)\n","  adding: pv/decoder-weights-improvement-3-0.18.hdf5 (deflated 17%)\n","  adding: pv/test10.png (deflated 2%)\n","  adding: pv/train15.png (deflated 2%)\n","  adding: pv/test26.png (deflated 2%)\n","  adding: pv/train21.png (deflated 2%)\n","  adding: pv/test9.png (deflated 2%)\n","  adding: pv/test11.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-85-0.04.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-45-0.04.hdf5 (deflated 16%)\n","  adding: pv/encoder-weights-improvement-50-0.05.hdf5 (deflated 18%)\n","  adding: pv/decoder-weights-improvement-2-0.65.hdf5 (deflated 16%)\n","  adding: pv/train16.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-12-0.06.hdf5 (deflated 13%)\n","  adding: pv/decoder-weights-improvement-18-0.19.hdf5 (deflated 20%)\n","  adding: pv/encoder-weights-improvement-70-0.05.hdf5 (deflated 24%)\n","  adding: pv/decoder-weights-improvement-5-0.24.hdf5 (deflated 18%)\n","  adding: pv/encoder-weights-improvement-24-0.06.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-98-0.04.hdf5 (deflated 26%)\n","  adding: pv/train25.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-9-0.21.hdf5 (deflated 19%)\n","  adding: pv/train23.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-76-0.04.hdf5 (deflated 25%)\n","  adding: pv/decoder-weights-improvement-16-0.21.hdf5 (deflated 20%)\n","  adding: pv/decoder-weights-improvement-28-0.18.hdf5 (deflated 22%)\n","  adding: pv/test5.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-11-0.19.hdf5 (deflated 19%)\n","  adding: pv/train12.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-17-0.21.hdf5 (deflated 20%)\n","  adding: pv/encoder-weights-improvement-40-0.05.hdf5 (deflated 15%)\n","  adding: pv/decoder-weights-improvement-12-0.20.hdf5 (deflated 20%)\n","  adding: pv/encoder-weights-improvement-69-0.05.hdf5 (deflated 24%)\n","  adding: pv/test6.png (deflated 2%)\n","  adding: pv/test24.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-33-0.05.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-06-0.09.hdf5 (deflated 10%)\n","  adding: pv/encoder-weights-improvement-71-0.05.hdf5 (deflated 24%)\n","  adding: pv/output_10.csv (deflated 48%)\n","  adding: pv/test12.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-59-0.05.hdf5 (deflated 23%)\n","  adding: pv/encoder-weights-improvement-52-0.05.hdf5 (deflated 20%)\n","  adding: pv/train20.png (deflated 3%)\n","  adding: pv/encoder-weights-improvement-32-0.06.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-58-0.05.hdf5 (deflated 23%)\n","  adding: pv/encoder-weights-improvement-54-0.05.hdf5 (deflated 21%)\n","  adding: pv/decoder-weights-improvement-6-0.21.hdf5 (deflated 18%)\n","  adding: pv/encoder-weights-improvement-47-0.04.hdf5 (deflated 16%)\n","  adding: pv/decoder-weights-improvement-8-0.20.hdf5 (deflated 19%)\n","  adding: pv/train9.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-88-0.05.hdf5 (deflated 25%)\n","  adding: pv/test4.png (deflated 2%)\n","  adding: pv/test0.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-51-0.05.hdf5 (deflated 19%)\n","  adding: pv/encoder-weights-improvement-53-0.05.hdf5 (deflated 21%)\n","  adding: pv/test15.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-22-0.06.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-42-0.07.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-03-0.06.hdf5 (deflated 8%)\n","  adding: pv/decoder-weights-improvement-14-0.21.hdf5 (deflated 20%)\n","  adding: pv/train18.png (deflated 3%)\n","  adding: pv/encoder-weights-improvement-60-0.05.hdf5 (deflated 23%)\n","  adding: pv/decoder-weights-improvement-1-0.63.hdf5 (deflated 15%)\n","  adding: pv/train17.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-89-0.04.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-36-0.05.hdf5 (deflated 15%)\n","  adding: pv/train14.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-0-0.18.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-04-0.07.hdf5 (deflated 8%)\n","  adding: pv/decoder-weights-improvement-24-0.21.hdf5 (deflated 21%)\n","  adding: pv/test2.png (deflated 3%)\n","  adding: pv/output_0.csv (deflated 47%)\n","  adding: pv/encoder-weights-improvement-20-0.06.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-07-0.09.hdf5 (deflated 11%)\n","  adding: pv/train8.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-61-0.05.hdf5 (deflated 23%)\n","  adding: pv/encoder-weights-improvement-01-0.05.hdf5 (deflated 6%)\n","  adding: pv/encoder-weights-improvement-57-0.05.hdf5 (deflated 22%)\n","  adding: pv/encoder-weights-improvement-17-0.07.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-87-0.04.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-14-0.07.hdf5 (deflated 14%)\n","  adding: pv/test18.png (deflated 2%)\n","  adding: pv/train3.png (deflated 2%)\n","  adding: pv/test7.png (deflated 2%)\n","  adding: pv/test16.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-67-0.05.hdf5 (deflated 24%)\n","  adding: pv/decoder-weights-improvement-25-0.17.hdf5 (deflated 21%)\n","  adding: pv/test19.png (deflated 3%)\n","  adding: pv/train10.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-39-0.04.hdf5 (deflated 15%)\n","  adding: pv/train4.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-22-0.20.hdf5 (deflated 21%)\n","  adding: pv/decoder-weights-improvement-27-0.18.hdf5 (deflated 21%)\n","  adding: pv/train13.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-13-0.25.hdf5 (deflated 20%)\n","  adding: pv/encoder-weights-improvement-81-0.05.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-15-0.07.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-31-0.05.hdf5 (deflated 15%)\n","  adding: pv/test22.png (deflated 3%)\n","  adding: pv/encoder-weights-improvement-35-0.05.hdf5 (deflated 15%)\n","  adding: pv/test8.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-27-0.04.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-29-0.05.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-90-0.04.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-56-0.05.hdf5 (deflated 22%)\n","  adding: pv/encoder-weights-improvement-77-0.05.hdf5 (deflated 25%)\n","  adding: pv/decoder-weights-improvement-20-0.21.hdf5 (deflated 20%)\n","  adding: pv/train0.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-93-0.04.hdf5 (deflated 26%)\n","  adding: pv/encoder-weights-improvement-18-0.08.hdf5 (deflated 14%)\n","  adding: pv/train6.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-72-0.05.hdf5 (deflated 24%)\n","  adding: pv/encoder-weights-improvement-79-0.05.hdf5 (deflated 25%)\n","  adding: pv/test3.png (deflated 2%)\n","  adding: pv/train27.png (deflated 2%)\n","  adding: pv/test13.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-34-0.08.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-91-0.04.hdf5 (deflated 26%)\n","  adding: pv/train7.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-23-0.06.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-99-0.04.hdf5 (deflated 26%)\n","  adding: pv/encoder-weights-improvement-25-0.06.hdf5 (deflated 14%)\n","  adding: pv/decoder-weights-improvement-26-0.18.hdf5 (deflated 21%)\n","  adding: pv/encoder-weights-improvement-80-0.04.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-11-0.10.hdf5 (deflated 13%)\n","  adding: pv/decoder-weights-improvement-19-0.20.hdf5 (deflated 20%)\n","  adding: pv/encoder-weights-improvement-83-0.05.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-68-0.05.hdf5 (deflated 24%)\n","  adding: pv/decoder-weights-improvement-10-0.23.hdf5 (deflated 19%)\n","  adding: pv/encoder-weights-improvement-16-0.08.hdf5 (deflated 14%)\n","  adding: pv/train22.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-46-0.05.hdf5 (deflated 16%)\n","  adding: pv/encoder-weights-improvement-96-0.04.hdf5 (deflated 26%)\n","  adding: pv/encoder-weights-improvement-86-0.04.hdf5 (deflated 25%)\n","  adding: pv/decoder-weights-improvement-21-0.20.hdf5 (deflated 21%)\n","  adding: pv/encoder-weights-improvement-65-0.04.hdf5 (deflated 24%)\n","  adding: pv/decoder-weights-improvement-7-0.13.hdf5 (deflated 18%)\n","  adding: pv/encoder-weights-improvement-55-0.05.hdf5 (deflated 22%)\n","  adding: pv/encoder-weights-improvement-02-0.08.hdf5 (deflated 7%)\n","  adding: pv/encoder-weights-improvement-49-0.04.hdf5 (deflated 18%)\n","  adding: pv/encoder-weights-improvement-09-0.07.hdf5 (deflated 12%)\n","  adding: pv/encoder-weights-improvement-94-0.04.hdf5 (deflated 26%)\n","  adding: pv/train2.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-73-0.05.hdf5 (deflated 24%)\n","  adding: pv/encoder-weights-improvement-41-0.05.hdf5 (deflated 15%)\n","  adding: pv/test21.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-05-0.06.hdf5 (deflated 9%)\n","  adding: pv/test1.png (deflated 2%)\n","  adding: pv/train26.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-10-0.08.hdf5 (deflated 13%)\n","  adding: pv/encoder-weights-improvement-38-0.04.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-43-0.09.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-78-0.05.hdf5 (deflated 25%)\n","  adding: pv/train11.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-64-0.06.hdf5 (deflated 23%)\n","  adding: pv/encoder-weights-improvement-19-0.06.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-28-0.05.hdf5 (deflated 15%)\n","  adding: pv/encoder-weights-improvement-37-0.04.hdf5 (deflated 15%)\n","  adding: pv/train1.png (deflated 5%)\n","  adding: pv/train5.png (deflated 2%)\n","  adding: pv/decoder-weights-improvement-4-0.19.hdf5 (deflated 17%)\n","  adding: pv/encoder-weights-improvement-84-0.05.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-13-0.07.hdf5 (deflated 13%)\n","  adding: pv/test23.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-74-0.05.hdf5 (deflated 24%)\n","  adding: pv/encoder-weights-improvement-21-0.07.hdf5 (deflated 14%)\n","  adding: pv/encoder-weights-improvement-82-0.05.hdf5 (deflated 25%)\n","  adding: pv/encoder-weights-improvement-95-0.04.hdf5 (deflated 26%)\n","  adding: pv/encoder-weights-improvement-48-0.04.hdf5 (deflated 17%)\n","  adding: pv/test25.png (deflated 2%)\n","  adding: pv/output_20.csv (deflated 49%)\n","  adding: pv/encoder-weights-improvement-26-0.04.hdf5 (deflated 15%)\n","  adding: pv/train19.png (deflated 2%)\n","  adding: pv/encoder-weights-improvement-63-0.06.hdf5 (deflated 23%)\n","  adding: pv/test20.png (deflated 3%)\n"],"name":"stdout"},{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f249c988f395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip -r pv.zip pv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pv.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"]}]},{"cell_type":"code","metadata":{"id":"1ywUouYh4rTY","colab_type":"code","outputId":"a70c9a84-0ac6-46f0-c1a6-aeb1d8d9ba61","executionInfo":{"status":"ok","timestamp":1580898510008,"user_tz":-60,"elapsed":139384,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!zip -r pv.zip pv"],"execution_count":0,"outputs":[{"output_type":"stream","text":["updating: pv/ (stored 0%)\n","updating: pv/train24.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-23-0.19.hdf5 (deflated 21%)\n","updating: pv/decoder-weights-improvement-15-0.21.hdf5 (deflated 20%)\n","updating: pv/encoder-weights-improvement-97-0.05.hdf5 (deflated 26%)\n","updating: pv/encoder-weights-improvement-44-0.10.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-92-0.05.hdf5 (deflated 26%)\n","updating: pv/encoder-weights-improvement-30-0.05.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-66-0.05.hdf5 (deflated 24%)\n","updating: pv/test14.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-08-0.08.hdf5 (deflated 11%)\n","updating: pv/test17.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-62-0.05.hdf5 (deflated 23%)\n","updating: pv/encoder-weights-improvement-75-0.04.hdf5 (deflated 24%)\n","updating: pv/test27.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-100-0.05.hdf5 (deflated 26%)\n","updating: pv/decoder-weights-improvement-3-0.18.hdf5 (deflated 17%)\n","updating: pv/test10.png (deflated 2%)\n","updating: pv/train15.png (deflated 2%)\n","updating: pv/test26.png (deflated 2%)\n","updating: pv/train21.png (deflated 2%)\n","updating: pv/test9.png (deflated 2%)\n","updating: pv/test11.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-85-0.04.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-45-0.04.hdf5 (deflated 16%)\n","updating: pv/encoder-weights-improvement-50-0.05.hdf5 (deflated 18%)\n","updating: pv/decoder-weights-improvement-2-0.65.hdf5 (deflated 16%)\n","updating: pv/train16.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-12-0.06.hdf5 (deflated 13%)\n","updating: pv/decoder-weights-improvement-18-0.19.hdf5 (deflated 20%)\n","updating: pv/encoder-weights-improvement-70-0.05.hdf5 (deflated 24%)\n","updating: pv/decoder-weights-improvement-5-0.24.hdf5 (deflated 18%)\n","updating: pv/encoder-weights-improvement-24-0.06.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-98-0.04.hdf5 (deflated 26%)\n","updating: pv/train25.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-9-0.21.hdf5 (deflated 19%)\n","updating: pv/train23.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-76-0.04.hdf5 (deflated 25%)\n","updating: pv/decoder-weights-improvement-16-0.21.hdf5 (deflated 20%)\n","updating: pv/decoder-weights-improvement-28-0.18.hdf5 (deflated 22%)\n","updating: pv/test5.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-11-0.19.hdf5 (deflated 19%)\n","updating: pv/train12.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-17-0.21.hdf5 (deflated 20%)\n","updating: pv/encoder-weights-improvement-40-0.05.hdf5 (deflated 15%)\n","updating: pv/decoder-weights-improvement-12-0.20.hdf5 (deflated 20%)\n","updating: pv/encoder-weights-improvement-69-0.05.hdf5 (deflated 24%)\n","updating: pv/test6.png (deflated 2%)\n","updating: pv/test24.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-33-0.05.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-06-0.09.hdf5 (deflated 10%)\n","updating: pv/encoder-weights-improvement-71-0.05.hdf5 (deflated 24%)\n","updating: pv/output_10.csv (deflated 48%)\n","updating: pv/test12.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-59-0.05.hdf5 (deflated 23%)\n","updating: pv/encoder-weights-improvement-52-0.05.hdf5 (deflated 20%)\n","updating: pv/train20.png (deflated 3%)\n","updating: pv/encoder-weights-improvement-32-0.06.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-58-0.05.hdf5 (deflated 23%)\n","updating: pv/encoder-weights-improvement-54-0.05.hdf5 (deflated 21%)\n","updating: pv/decoder-weights-improvement-6-0.21.hdf5 (deflated 18%)\n","updating: pv/encoder-weights-improvement-47-0.04.hdf5 (deflated 16%)\n","updating: pv/decoder-weights-improvement-8-0.20.hdf5 (deflated 19%)\n","updating: pv/train9.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-88-0.05.hdf5 (deflated 25%)\n","updating: pv/test4.png (deflated 2%)\n","updating: pv/test0.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-51-0.05.hdf5 (deflated 19%)\n","updating: pv/encoder-weights-improvement-53-0.05.hdf5 (deflated 21%)\n","updating: pv/test15.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-22-0.06.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-42-0.07.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-03-0.06.hdf5 (deflated 8%)\n","updating: pv/decoder-weights-improvement-14-0.21.hdf5 (deflated 20%)\n","updating: pv/train18.png (deflated 3%)\n","updating: pv/encoder-weights-improvement-60-0.05.hdf5 (deflated 23%)\n","updating: pv/decoder-weights-improvement-1-0.63.hdf5 (deflated 15%)\n","updating: pv/train17.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-89-0.04.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-36-0.05.hdf5 (deflated 15%)\n","updating: pv/train14.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-0-0.18.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-04-0.07.hdf5 (deflated 8%)\n","updating: pv/decoder-weights-improvement-24-0.21.hdf5 (deflated 21%)\n","updating: pv/test2.png (deflated 3%)\n","updating: pv/output_0.csv (deflated 47%)\n","updating: pv/encoder-weights-improvement-20-0.06.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-07-0.09.hdf5 (deflated 11%)\n","updating: pv/train8.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-61-0.05.hdf5 (deflated 23%)\n","updating: pv/encoder-weights-improvement-01-0.05.hdf5 (deflated 6%)\n","updating: pv/encoder-weights-improvement-57-0.05.hdf5 (deflated 22%)\n","updating: pv/encoder-weights-improvement-17-0.07.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-87-0.04.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-14-0.07.hdf5 (deflated 14%)\n","updating: pv/test18.png (deflated 2%)\n","updating: pv/train3.png (deflated 2%)\n","updating: pv/test7.png (deflated 2%)\n","updating: pv/test16.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-67-0.05.hdf5 (deflated 24%)\n","updating: pv/decoder-weights-improvement-25-0.17.hdf5 (deflated 21%)\n","updating: pv/test19.png (deflated 3%)\n","updating: pv/train10.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-39-0.04.hdf5 (deflated 15%)\n","updating: pv/train4.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-22-0.20.hdf5 (deflated 21%)\n","updating: pv/decoder-weights-improvement-27-0.18.hdf5 (deflated 21%)\n","updating: pv/train13.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-13-0.25.hdf5 (deflated 20%)\n","updating: pv/encoder-weights-improvement-81-0.05.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-15-0.07.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-31-0.05.hdf5 (deflated 15%)\n","updating: pv/test22.png (deflated 3%)\n","updating: pv/encoder-weights-improvement-35-0.05.hdf5 (deflated 15%)\n","updating: pv/test8.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-27-0.04.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-29-0.05.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-90-0.04.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-56-0.05.hdf5 (deflated 22%)\n","updating: pv/encoder-weights-improvement-77-0.05.hdf5 (deflated 25%)\n","updating: pv/decoder-weights-improvement-20-0.21.hdf5 (deflated 20%)\n","updating: pv/train0.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-93-0.04.hdf5 (deflated 26%)\n","updating: pv/encoder-weights-improvement-18-0.08.hdf5 (deflated 14%)\n","updating: pv/train6.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-72-0.05.hdf5 (deflated 24%)\n","updating: pv/encoder-weights-improvement-79-0.05.hdf5 (deflated 25%)\n","updating: pv/test3.png (deflated 2%)\n","updating: pv/train27.png (deflated 2%)\n","updating: pv/test13.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-34-0.08.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-91-0.04.hdf5 (deflated 26%)\n","updating: pv/train7.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-23-0.06.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-99-0.04.hdf5 (deflated 26%)\n","updating: pv/encoder-weights-improvement-25-0.06.hdf5 (deflated 14%)\n","updating: pv/decoder-weights-improvement-26-0.18.hdf5 (deflated 21%)\n","updating: pv/encoder-weights-improvement-80-0.04.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-11-0.10.hdf5 (deflated 13%)\n","updating: pv/decoder-weights-improvement-19-0.20.hdf5 (deflated 20%)\n","updating: pv/encoder-weights-improvement-83-0.05.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-68-0.05.hdf5 (deflated 24%)\n","updating: pv/decoder-weights-improvement-10-0.23.hdf5 (deflated 19%)\n","updating: pv/encoder-weights-improvement-16-0.08.hdf5 (deflated 14%)\n","updating: pv/train22.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-46-0.05.hdf5 (deflated 16%)\n","updating: pv/encoder-weights-improvement-96-0.04.hdf5 (deflated 26%)\n","updating: pv/encoder-weights-improvement-86-0.04.hdf5 (deflated 25%)\n","updating: pv/decoder-weights-improvement-21-0.20.hdf5 (deflated 21%)\n","updating: pv/encoder-weights-improvement-65-0.04.hdf5 (deflated 24%)\n","updating: pv/decoder-weights-improvement-7-0.13.hdf5 (deflated 18%)\n","updating: pv/encoder-weights-improvement-55-0.05.hdf5 (deflated 22%)\n","updating: pv/encoder-weights-improvement-02-0.08.hdf5 (deflated 7%)\n","updating: pv/encoder-weights-improvement-49-0.04.hdf5 (deflated 18%)\n","updating: pv/encoder-weights-improvement-09-0.07.hdf5 (deflated 12%)\n","updating: pv/encoder-weights-improvement-94-0.04.hdf5 (deflated 26%)\n","updating: pv/train2.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-73-0.05.hdf5 (deflated 24%)\n","updating: pv/encoder-weights-improvement-41-0.05.hdf5 (deflated 15%)\n","updating: pv/test21.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-05-0.06.hdf5 (deflated 9%)\n","updating: pv/test1.png (deflated 2%)\n","updating: pv/train26.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-10-0.08.hdf5 (deflated 13%)\n","updating: pv/encoder-weights-improvement-38-0.04.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-43-0.09.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-78-0.05.hdf5 (deflated 25%)\n","updating: pv/train11.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-64-0.06.hdf5 (deflated 23%)\n","updating: pv/encoder-weights-improvement-19-0.06.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-28-0.05.hdf5 (deflated 15%)\n","updating: pv/encoder-weights-improvement-37-0.04.hdf5 (deflated 15%)\n","updating: pv/train1.png (deflated 5%)\n","updating: pv/train5.png (deflated 2%)\n","updating: pv/decoder-weights-improvement-4-0.19.hdf5 (deflated 17%)\n","updating: pv/encoder-weights-improvement-84-0.05.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-13-0.07.hdf5 (deflated 13%)\n","updating: pv/test23.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-74-0.05.hdf5 (deflated 24%)\n","updating: pv/encoder-weights-improvement-21-0.07.hdf5 (deflated 14%)\n","updating: pv/encoder-weights-improvement-82-0.05.hdf5 (deflated 25%)\n","updating: pv/encoder-weights-improvement-95-0.04.hdf5 (deflated 26%)\n","updating: pv/encoder-weights-improvement-48-0.04.hdf5 (deflated 17%)\n","updating: pv/test25.png (deflated 2%)\n","updating: pv/output_20.csv (deflated 49%)\n","updating: pv/encoder-weights-improvement-26-0.04.hdf5 (deflated 15%)\n","updating: pv/train19.png (deflated 2%)\n","updating: pv/encoder-weights-improvement-63-0.06.hdf5 (deflated 23%)\n","updating: pv/test20.png (deflated 3%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"26piBS29Db_O","colab_type":"code","outputId":"f587e2f1-32bd-4f2a-e27b-e3220ee1715c","executionInfo":{"status":"error","timestamp":1580986654293,"user_tz":-60,"elapsed":3931,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","    TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","    When encoder is traine completely, merged model exhibits much better\n","    performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","    encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","    decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","%tensorflow_version 2.x\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","\n","import tensorflow as tf\n","\n","import matplotlib\n","\n","tf.test.gpu_device_name()\n","\n","with tf.device('/device:GPU:0'):\n","  # Different configurations for different computation structures\n","  # gpu = input('On Server?(y/n) : ')\n","  gpu = 'n'\n","  if(gpu == 'y'):\n","      # Ask for the parameters\n","      encoderEpoch    = 10\n","      decoderEpoch    = 5\n","      numNeuron       = 128\n","      batchSize       = 1\n","\n","      forecastHorizon = input('T - M         : ')\n","      directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = input('Data size     : ')\n","      M               = input('M             : ')\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","      stateful = True\n","\n","\n","      # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","  else:\n","      # Ask for the parameters\n","      encoderEpoch    = 50\n","      decoderEpoch    = 300\n","      numNeuron       = 512\n","      batchSize       = 1\n","\n","      forecastHorizon = '24'\n","      directoryName   = '/content/drive/My Drive/Colab Notebooks/load' # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = '1440'\n","      M               = '24'\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","\n","\n","      stateful = False\n","\n","\n","\n","      os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","  import matplotlib.pyplot as plt\n","\n","\n","\n","\n","  #################### Code starts\n","  # Parameters for LSTM input (we need to know the input shape)\n","  # These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","  num_months = 0\n","  num_days   = 0\n","  num_hours  = 0\n","\n","  def plot(figNum, originalData, lstmOuts, customTag=''):\n","      # lstmOuts -> predictions inverse transformed\n","      # originalData -> data read from file\n","\n","      # forecastLong : forecast with given (T - M)\n","      forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      listStart = -50\n","      listEnd   = -2\n","      plt.plot(originalData[listStart:listEnd])\n","      plt.plot(forecastLong[listStart:listEnd])\n","      plt.plot(persis[listStart+1:listEnd+1])\n","      plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","      title = ''\n","      title = title + customTag + '\\n'\n","      title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","      title = title + ('dataSize = ' + str(dataSize) + ', ')\n","      title = title + ('M = ' + str(M) + ', ')\n","      title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","      title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","      # title = title + ('loss = ' + str(loss) + ', ')\n","      # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","      title = title + ('(T - M) = ' + str(T-M) + ', ')\n","      title = title + ('currentEpoch = ' + str(figNum) + '')\n","      plt.title(title, fontsize=6)\n","\n","      plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","      plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","      plt.savefig(directoryName + '/' + customTag + str(figNum) + '.png', bbox_inches=\"tight\")\n","      plt.cla()\n","\n","\n","  def noneNum(allorders):\n","      # Replace None's in a python list with 0's and return the output\n","      allorders = list(allorders)\n","      output = [i if i is not None else 0 for i in allorders]\n","\n","      return output\n","\n","  def calculateError(originalData, lstmOuts):\n","      forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      # Calculate error\n","      endIndex = -(T - M) + 1\n","      if endIndex == 0:\n","          endIndex = originalData.shape[0]\n","\n","      text = ''\n","\n","      maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MAE my model     : \" + str(maemym)\n","      text = text + '\\n'\n","      text = text + \"MAE persistence  : \" + str(maepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MSE my model     : \" + str(msemym)\n","      text = text + '\\n'\n","      text = text + \"MSE persistence  : \" + str(msepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      # # TODO - can be replaced by tensorflow\n","      # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # text = text + \"MAPE my model    : \" + str(mapemym)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","      \n","      # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # with tensorflow.Session() as sess:\n","          # mapemym_val = mapemym.eval()\n","          # mapepers_val = mapepers.eval()\n","      \n","      # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","\n","\n","      return text\n","\n","  #def mean_absolute_percentage_error(y_true, y_pred):\n","  #    # Not implemented in sklearn\n","  #\n","  #    # Not recommended to use since negative values\n","  #    # create negative losses\n","  #\n","  #    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","  def readData(dataType):\n","      # if dataType == 'train':\n","      #     data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_train.csv',sep=',')\n","      # elif dataType == 'valid':\n","      data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_valid.csv',sep=',')\n","\n","      outputDat = np.array(data.total_pv.iloc[:len(data)])\n","      date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","      return outputDat, date_time\n","\n","\n","  # Taken from test3.py\n","  # Taken from test3.py\n","  def prepareDataTraining(Xa, date_time):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","      \n","      dataSize = len(Xa)\n","\n","      scaler = MinMaxScaler(feature_range =(-1,1))\n","      X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","      \n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","      onehot_day_of_week   = OneHotEncoder(sparse=False)\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","      print(type(all_hours))\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","  # Taken from test3.py\n","  def prepareDataTesting(Xa, date_time):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","\n","      dataSize = len(Xa)\n","      scaler = MinMaxScaler(feature_range =(-1,1))\n","      X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","      onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","  global forecasts\n","\n","  ############# Run code\n","  # trainData, train_date_time = readData('train')\n","  testData,  test_date_time  = readData('valid')\n","\n","  # trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","  testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time)\n","\n","\n","\n","  ### Tensorboard modelling\n","  # logdirEnc = \"load_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  # logdirDec = \"/content/drive/My Drive/Colab Notebooks/load/decoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  # tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","  # tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","  ### Create Encoder model\n","\n","  # Model layers\n","  num_features_encoder = 1 + (num_months + num_days + num_hours)\n","  inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","  encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful, activation = \"relu\")\n","  encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  #encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  encoderDense = TimeDistributed(Dense(1))\n","\n","\n","  # Model connections\n","  encoderLSTMOutputs  = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","  encoderLSTM2Outputs, _, _      = encoderLSTM2(encoderLSTMOutputs)\n","  #encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\n","  encoderOutputs           = encoderDense(encoderLSTM2Outputs)\n","\n","\n","  encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","  encoderModel.compile(optimizer='adam', loss='mse')\n","\n","  # encoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/encoder-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n","  # encoderCheckpoint = ModelCheckpoint(encoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","\n","  # print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[encoderCheckpoint,tensorboardCallbackEncoder])\n","  encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/encoder_pv_e37.hdf5')\n","\n","\n","  ### Create merged (Encoder + Decoder) model\n","\n","  # Model layers\n","  num_features_decoder   = (num_months + num_days + num_hours)\n","  inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","  decoderBroadcasted     = Lambda(lambda x: broadcast_to(x, [batchSize, T-M, numNeuron]))\n","  decoderConcatenated    = Lambda(lambda x: concat(x, axis=2))\n","  decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","  decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","  decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","  #decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","  decoderDense           = TimeDistributed(Dense(1))\n","  layerV                 = Dense(numNeuron)\n","  layerVActivation       = Activation('relu')\n","  layerV_prime           = Dense(numNeuron)\n","  layerV_primeActivation = Activation('relu')\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","  # Map encoder hidden state final to some vector(simple layer)\n","  c = layerVActivation(layerV(encoder_h))\n","  # Remap the c vector to h_prime\n","  h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","  # Use h_prime as input to other layers\n","  c_broadcasted              = decoderBroadcasted(c)\n","  decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","  # decoderStateZeros : just zeros to feed to the internal state\n","  decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","  decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","  decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","  #decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","  decoderOutputs     = decoderDense(decoderLSTMOutputs2)\n","\n","\n","  mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","  mergedModel.compile(optimizer='adam', loss='mse')\n","  print('Training merged(encoder + decoder) model...');\n","\n","  outputs = list()\n","\n","  MSEs = list()  \n","  Y = np.reshape(YtestDecoder,(384,24))\n","  mypath = 'drive/My Drive/Colab Notebooks/pv_encoder_e37/models/'\n","  modelList = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","  losses = [float(modelList[i][-9:-5]) for i in range(len(modelList))]\n","  losses = pd.DataFrame(losses,columns=['mse'])\n","  best = list(losses.sort_values(by=['mse']).index[0:30])\n","  for i in best:\n","    mod = str(mypath + modelList[i])\n","    mergedModel = load_model(mod)\n","    lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","    lstmOuts = np.reshape(lstmOuts,(384,24))\n","    \n","    e = mean_squared_error(Y, lstmOuts)\n","    print('epoch ' + str(i) + ': ',e)\n","    MSEs.append(e)\n","\n","      \n","\n","  #np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-3e343264398a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   \u001b[0;31m# Model connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m   \u001b[0mencoderLSTMOutputs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputEncoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#encoders_c is already equal to encoderLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m   \u001b[0mencoderLSTM2Outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mencoderLSTM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoderLSTMOutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m   \u001b[0;31m#encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m         caching_device=default_caching_device)\n\u001b[0m\u001b[1;32m   2302\u001b[0m     self.recurrent_kernel = self.add_weight(\n\u001b[1;32m   2303\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2594\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2597\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m     return variables.RefVariable(\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1409\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m   def _init_from_args(self,\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1540\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1542\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1544\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    121\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m    786\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     return op(\n\u001b[0;32m--> 788\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mminval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: /job:localhost/replica:0/task:0/device:GPU:0 unknown device."]}]},{"cell_type":"code","metadata":{"id":"bexzaeBTJgf0","colab_type":"code","outputId":"4b7fb890-4c8d-44a5-f311-a19d16c73675","colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"status":"ok","timestamp":1581325451556,"user_tz":-60,"elapsed":623898,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","    TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","    When encoder is traine completely, merged model exhibits much better\n","    performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","    encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","    decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","%tensorflow_version 2.x\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","\n","import tensorflow as tf\n","\n","import matplotlib\n","\n","#tf.test.gpu_device_name()\n","\n","\n","  # Different configurations for different computation structures\n","  # gpu = input('On Server?(y/n) : ')\n","gpu = 'n'\n","if(gpu == 'y'):\n","    # Ask for the parameters\n","    encoderEpoch    = 10\n","    decoderEpoch    = 5\n","    numNeuron       = 128\n","    batchSize       = 1\n","\n","    forecastHorizon = input('T - M         : ')\n","    directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = input('Data size     : ')\n","    M               = input('M             : ')\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","    stateful = True\n","\n","\n","    # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","else:\n","    # Ask for the parameters\n","    encoderEpoch    = 50\n","    decoderEpoch    = 300\n","    numNeuron       = 512\n","    batchSize       = 1\n","\n","    forecastHorizon = '24'\n","    directoryName   = '/content/drive/My Drive/Colab Notebooks/load' # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = '1440'\n","    M               = '24'\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","\n","\n","    stateful = False\n","\n","\n","\n","    os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","\n","#################### Code starts\n","# Parameters for LSTM input (we need to know the input shape)\n","# These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","num_months = 0\n","num_days   = 0\n","num_hours  = 0\n","\n","def plot(figNum, originalData, lstmOuts, customTag=''):\n","    # lstmOuts -> predictions inverse transformed\n","    # originalData -> data read from file\n","\n","    # forecastLong : forecast with given (T - M)\n","    forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    listStart = -50\n","    listEnd   = -2\n","    plt.plot(originalData[listStart:listEnd])\n","    plt.plot(forecastLong[listStart:listEnd])\n","    plt.plot(persis[listStart+1:listEnd+1])\n","    plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","    title = ''\n","    title = title + customTag + '\\n'\n","    title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","    title = title + ('dataSize = ' + str(dataSize) + ', ')\n","    title = title + ('M = ' + str(M) + ', ')\n","    title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","    title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","    # title = title + ('loss = ' + str(loss) + ', ')\n","    # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","    title = title + ('(T - M) = ' + str(T-M) + ', ')\n","    title = title + ('currentEpoch = ' + str(figNum) + '')\n","    plt.title(title, fontsize=6)\n","\n","    plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","    plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","    plt.savefig(directoryName + '/' + customTag + str(figNum) + '.png', bbox_inches=\"tight\")\n","    plt.cla()\n","\n","\n","def noneNum(allorders):\n","    # Replace None's in a python list with 0's and return the output\n","    allorders = list(allorders)\n","    output = [i if i is not None else 0 for i in allorders]\n","\n","    return output\n","\n","def calculateError(originalData, lstmOuts):\n","    forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    # Calculate error\n","    endIndex = -(T - M) + 1\n","    if endIndex == 0:\n","        endIndex = originalData.shape[0]\n","\n","    text = ''\n","\n","    maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MAE my model     : \" + str(maemym)\n","    text = text + '\\n'\n","    text = text + \"MAE persistence  : \" + str(maepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MSE my model     : \" + str(msemym)\n","    text = text + '\\n'\n","    text = text + \"MSE persistence  : \" + str(msepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    # # TODO - can be replaced by tensorflow\n","    # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # text = text + \"MAPE my model    : \" + str(mapemym)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","    \n","    # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # with tensorflow.Session() as sess:\n","        # mapemym_val = mapemym.eval()\n","        # mapepers_val = mapepers.eval()\n","    \n","    # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","\n","\n","    return text\n","\n","#def mean_absolute_percentage_error(y_true, y_pred):\n","#    # Not implemented in sklearn\n","#\n","#    # Not recommended to use since negative values\n","#    # create negative losses\n","#\n","#    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","def readData(dataType):\n","    # if dataType == 'train':\n","    #     data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_train.csv',sep=',')\n","    # elif dataType == 'valid':\n","    data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_valid.csv',sep=',')\n","\n","    outputDat = np.array(data.total_pv.iloc[:len(data)])\n","    date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","    return outputDat, date_time\n","\n","\n","# Taken from test3.py\n","# Taken from test3.py\n","def prepareDataTraining(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","    \n","    dataSize = len(Xa)\n","\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","    \n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","    onehot_day_of_week   = OneHotEncoder(sparse=False)\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","    print(type(all_hours))\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","# Taken from test3.py\n","def prepareDataTesting(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","\n","    dataSize = len(Xa)\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","    onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","global forecasts\n","\n","############# Run code\n","# trainData, train_date_time = readData('train')\n","testData,  test_date_time  = readData('valid')\n","\n","# trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time)\n","\n","\n","\n","### Tensorboard modelling\n","# logdirEnc = \"load_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# logdirDec = \"/content/drive/My Drive/Colab Notebooks/load/decoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","# tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","### Create Encoder model\n","\n","# Model layers\n","num_features_encoder = 1 + (num_months + num_days + num_hours)\n","inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful, activation = \"relu\")\n","encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","#encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","encoderDense = TimeDistributed(Dense(1))\n","\n","\n","# Model connections\n","encoderLSTMOutputs  = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","encoderLSTM2Outputs, _, _      = encoderLSTM2(encoderLSTMOutputs)\n","#encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\n","encoderOutputs           = encoderDense(encoderLSTM2Outputs)\n","\n","\n","encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","encoderModel.compile(optimizer='adam', loss='mse')\n","\n","# encoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/encoder-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n","# encoderCheckpoint = ModelCheckpoint(encoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","\n","# print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[encoderCheckpoint,tensorboardCallbackEncoder])\n","encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/encoder_pv_e37.hdf5')\n","\n","\n","### Create merged (Encoder + Decoder) model\n","\n","# Model layers\n","num_features_decoder   = (num_months + num_days + num_hours)\n","inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","decoderBroadcasted     = Lambda(lambda x: broadcast_to(x, [batchSize, T-M, numNeuron]))\n","decoderConcatenated    = Lambda(lambda x: concat(x, axis=2))\n","decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","#decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","decoderDense           = TimeDistributed(Dense(1))\n","layerV                 = Dense(numNeuron)\n","layerVActivation       = Activation('relu')\n","layerV_prime           = Dense(numNeuron)\n","layerV_primeActivation = Activation('relu')\n","\n","# Model connections\n","encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","# Map encoder hidden state final to some vector(simple layer)\n","c = layerVActivation(layerV(encoder_h))\n","# Remap the c vector to h_prime\n","h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","# Use h_prime as input to other layers\n","c_broadcasted              = decoderBroadcasted(c)\n","decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","# decoderStateZeros : just zeros to feed to the internal state\n","decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","#decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","decoderOutputs     = decoderDense(decoderLSTMOutputs2)\n","\n","\n","mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","mergedModel.compile(optimizer='adam', loss='mse')\n","print('Training merged(encoder + decoder) model...');\n","\n","outputs = list()\n","\n","MSEs = list()  \n","Y = np.reshape(YtestDecoder,(384,24))\n","mypath = 'drive/My Drive/Colab Notebooks/pv_encoder_e37/models/'\n","modelList = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","losses = [float(modelList[i][-9:-5]) for i in range(len(modelList))]\n","losses = pd.DataFrame(losses,columns=['mse'])\n","best = list(losses.sort_values(by=['mse']).index[0:30])\n","for i in best:\n","  mod = str(mypath + modelList[i])\n","  mergedModel = load_model(mod)\n","  lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","  lstmOuts = np.reshape(lstmOuts,(384,24))\n","  \n","  e = mean_squared_error(Y, lstmOuts)\n","  print('epoch ' + str(i) + ': ',e)\n","  MSEs.append(e)\n","\n","    \n","\n","#np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","Training merged(encoder + decoder) model...\n","epoch 1:  0.09668476320874168\n","epoch 3:  0.10426410925616214\n","epoch 12:  0.10762308000065963\n","epoch 0:  0.11891348772996778\n","epoch 21:  0.11546608420792058\n","epoch 15:  0.11580062990971911\n","epoch 13:  0.12262000012082423\n","epoch 11:  0.11905145059083742\n","epoch 4:  0.1321702164765114\n","epoch 19:  0.1311556570619017\n","epoch 20:  0.12967398906250907\n","epoch 22:  0.13722345011437506\n","epoch 5:  0.14400974516216605\n","epoch 16:  0.14334022071097416\n","epoch 25:  0.1421712167797754\n","epoch 17:  0.15103330407983676\n","epoch 2:  0.15331079131148803\n","epoch 32:  0.15788147035156266\n","epoch 27:  0.1627223891711949\n","epoch 26:  0.15901453177518046\n","epoch 24:  0.15535285669719212\n","epoch 29:  0.16909496814323946\n","epoch 240:  0.17498940226764925\n","epoch 146:  0.17319802018382427\n","epoch 248:  0.16801410561018548\n","epoch 190:  0.1801949229746016\n","epoch 129:  0.18459129309944086\n","epoch 126:  0.1843110430503109\n","epoch 208:  0.1779888231330088\n","epoch 210:  0.18473173402998752\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KIaK9lUtKJyn","colab_type":"code","colab":{}},"source":["MSE = pd.DataFrame(MSEs, columns=['mse'],index = [list(losses.sort_values(by=['mse']).index[0:30])])\n","MSE.sort_values(by=['mse'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FG5fvHK_fVb2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"496dbf3e-b6f7-4295-c104-f33d8c5e0fad","executionInfo":{"status":"error","timestamp":1581331525407,"user_tz":-60,"elapsed":2231793,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","    TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","    When encoder is traine completely, merged model exhibits much better\n","    performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","    encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","    decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","\n","import tensorflow as tf\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","import matplotlib\n","\n","# Different configurations for different computation structures\n","# gpu = input('On Server?(y/n) : ')\n","gpu = 'n'\n","if(gpu == 'y'):\n","    # Ask for the parameters\n","    encoderEpoch    = 10\n","    decoderEpoch    = 5\n","    numNeuron       = 128\n","    batchSize       = 1\n","\n","    forecastHorizon = input('T - M         : ')\n","    directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = input('Data size     : ')\n","    M               = input('M             : ')\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","    stateful = True\n","\n","\n","    # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","else:\n","    # Ask for the parameters\n","    encoderEpoch    = 100\n","    decoderEpoch    = 300\n","    numNeuron       = 512\n","    batchSize       = 1\n","\n","    forecastHorizon = '24'\n","    directoryName   = '/content/drive/My Drive/Colab Notebooks/pv_forecasts' # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = '1440'\n","    M               = '24'\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","\n","\n","    stateful = False\n","\n","\n","\n","    os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","\n","#################### Code starts\n","# Parameters for LSTM input (we need to know the input shape)\n","# These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","num_months = 0\n","num_days   = 0\n","num_hours  = 0\n","\n","def plot(figNum, originalData, lstmOuts, customTag=''):\n","    # lstmOuts -> predictions inverse transformed\n","    # originalData -> data read from file\n","\n","    # forecastLong : forecast with given (T - M)\n","    forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    listStart = -25\n","    listEnd   = -1\n","    plt.plot(originalData[listStart:listEnd])\n","    plt.plot(forecastLong[listStart:listEnd])\n","#    plt.plot(persis[listStart+1:listEnd+1])\n","#    plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","    title = ''\n","    title = title + customTag + '\\n'\n","    title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","    title = title + ('dataSize = ' + str(dataSize) + ', ')\n","    title = title + ('M = ' + str(M) + ', ')\n","    title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","    title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","    # title = title + ('loss = ' + str(loss) + ', ')\n","    # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","    title = title + ('(T - M) = ' + str(T-M) + ', ')\n","    title = title + ('currentEpoch = ' + str(figNum) + '')\n","    plt.title(title, fontsize=6)\n","\n","#    plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","    plt.legend(['Original Data', 'LSTM'])\n","    plt.savefig(directoryName + '/' + customTag + str(figNum) + 'test.png', bbox_inches=\"tight\")\n","    plt.cla()\n","    return forecastLong[listStart:listEnd],originalData[listStart:listEnd]\n","\n","\n","def noneNum(allorders):\n","    # Replace None's in a python list with 0's and return the output\n","    allorders = list(allorders)\n","    output = [i if i is not None else 0 for i in allorders]\n","\n","    return output\n","\n","def calculateError(originalData, lstmOuts):\n","    forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    # Calculate error\n","    endIndex = -(T - M) + 1\n","    if endIndex == 0:\n","        endIndex = originalData.shape[0]\n","\n","    text = ''\n","\n","    maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MAE my model     : \" + str(maemym)\n","    text = text + '\\n'\n","    text = text + \"MAE persistence  : \" + str(maepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MSE my model     : \" + str(msemym)\n","    text = text + '\\n'\n","    text = text + \"MSE persistence  : \" + str(msepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    # # TODO - can be replaced by tensorflow\n","    # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # text = text + \"MAPE my model    : \" + str(mapemym)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","    \n","    # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # with tensorflow.Session() as sess:\n","        # mapemym_val = mapemym.eval()\n","        # mapepers_val = mapepers.eval()\n","    \n","    # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","\n","\n","    return text\n","\n","#def mean_absolute_percentage_error(y_true, y_pred):\n","#    # Not implemented in sklearn\n","#\n","#    # Not recommended to use since negative values\n","#    # create negative losses\n","#\n","#    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","def readData(dataType):\n","    if dataType == 'train':\n","        data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_train.csv',sep=',')\n","    elif dataType == 'test':\n","        data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/pv_test.csv',sep=',')\n","\n","    outputDat = np.array(data.total_pv.iloc[:len(data)])\n","    date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","    return outputDat, date_time\n","\n","\n","# Taken from test3.py\n","# Taken from test3.py\n","def prepareDataTraining(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","    \n","    dataSize = len(Xa)\n","\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","    \n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","    onehot_day_of_week   = OneHotEncoder(sparse=False)\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","    print(type(all_hours))\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","# Taken from test3.py\n","def prepareDataTesting(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","\n","    dataSize = len(Xa)\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","    onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","global forecasts\n","\n","############# Run code\n","trainData, train_date_time = readData('train')\n","testData,  test_date_time  = readData('test')\n","\n","trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time)\n","\n","\n","### Tensorboard modelling\n","#logdirEnc = \"pv_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","logdirDec = \"pv_final\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","### Create Encoder model\n","\n","# # Model layers\n","# num_features_encoder = 1 + (num_months + num_days + num_hours)\n","# inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","# encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful, activation = \"relu\")\n","# #encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","# #encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","# encoderDense = TimeDistributed(Dense(1))\n","\n","\n","# # Model connections\n","# encoderLSTMOutputs, _, _  = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","# #encoderLSTM2Outputs      = encoderLSTM2(encoderLSTMOutputs)\n","# #encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\n","# encoderOutputs           = encoderDense(encoderLSTMOutputs)\n","\n","\n","# encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","# encoderModel.compile(optimizer='adam', loss='mse')\n","#print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,verbose = 0,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[tensorboardCallbackEncoder])\n","encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/pv_encoder.hdf5')\n","\n","\n","# ### Create merged (Encoder + Decoder) model\n","\n","# # Model layers\n","# num_features_decoder   = (num_months + num_days + num_hours)\n","# inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","# decoderBroadcasted     = Lambda(lambda x: tf.broadcast_to(x, [batchSize, T-M, numNeuron]))\n","# decoderConcatenated    = Lambda(lambda x: tf.concat(x, axis=2))\n","# decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","# decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","# decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","# #decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","# decoderDense           = TimeDistributed(Dense(1))\n","# layerV                 = Dense(numNeuron)\n","# layerVActivation       = Activation('relu')\n","# layerV_prime           = Dense(numNeuron)\n","# layerV_primeActivation = Activation('relu')\n","\n","# # Model connections\n","# encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","# # Map encoder hidden state final to some vector(simple layer)\n","# c = layerVActivation(layerV(encoder_h))\n","# # Remap the c vector to h_prime\n","# h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","# # Use h_prime as input to other layers\n","# c_broadcasted              = decoderBroadcasted(c)\n","# decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","# # decoderStateZeros : just zeros to feed to the internal state\n","# decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","# decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","# decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","# #decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","# decoderOutputs     = decoderDense(decoderLSTMOutputs2)\n","\n","foCa = list()\n","orDa = list()\n","# mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","# mergedModel.compile(optimizer='adam', loss='mse')\n","print('Training merged(encoder + decoder) model...');\n","# mergedModel = load_model('/content/drive/My Drive/Colab Notebooks/pv_encoder_2.hdf5')\n","#outputs = list()\n","mypath = '/content/drive/My Drive/Colab Notebooks/pv_encoder_e37/models/'\n","modelList = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","i = 46\n","mod = mypath + modelList[i]\n","mergedModel = load_model(mod)\n","#lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","#lstmOuts = mergedModel.predict([XtrainEncoder, XtrainDecoder])\n","lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","#    Tscaler = MinMaxScaler(feature_range =(-1,1))\n","#    lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","lstmOuts = trainScaler.inverse_transform(lstmOuts)\n","fc, od = plot(i, testData, lstmOuts, 'test')\n","#fc, od = plot(i, trainData, lstmOuts, 'test')\n","\n","foCa.append(fc)\n","orDa.append(od)\n","\n","    \n","    #error_test = calculateError(testData, lstmOuts)\n","    #outputs.append(lstmOuts[-1])\n","    #outputsDF = pd.DataFrame(outputs)\n","    #if i%10 == 0:\n","    #    outputsDF.to_csv(directoryName + '/' + 'output_' + str(i) + '.csv', sep = ',')\n","    #    outputs = list()\n","    \n","#    print('train errors: ', error_train)\n","#    print('test errors: ',error_test)\n","        \n","#np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","Training merged(encoder + decoder) model...\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ae6fde9446fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmypath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodelList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m   \u001b[0mmergedModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m   \u001b[0mlstmOuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmergedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXtestEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtestDecoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;31m#lstmOuts = mergedModel.predict([XtrainEncoder, XtrainDecoder])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    141\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    142\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    188\u001b[0m           \u001b[0moptimizer_weight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_optimizer_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_weight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             logging.warning('Error in loading the saved optimizer '\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/adam.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m       \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    755\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   3183\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3184\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3186\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"_gEKABq3kc-e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"585af202-7c55-4a12-f652-cc0cdded12fe","executionInfo":{"status":"ok","timestamp":1581331546093,"user_tz":-60,"elapsed":727,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}}},"source":["list(foCa[46])"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.0698564052581787,\n"," 0.35957014560699463,\n"," 0.641507089138031,\n"," 0.590233325958252,\n"," 0.533731997013092,\n"," 0.8179044127464294,\n"," 1.2503305673599243,\n"," 1.9891387224197388,\n"," 4.05208158493042,\n"," 6.880776405334473,\n"," 9.109617233276367,\n"," 11.40361499786377,\n"," 12.388677597045898,\n"," 13.7584810256958,\n"," 13.839057922363281,\n"," 12.573826789855957,\n"," 10.73514461517334,\n"," 8.290668487548828,\n"," 5.498876571655273,\n"," 2.1390068531036377,\n"," 1.2961009740829468,\n"," 1.2369197607040405,\n"," 1.0896867513656616,\n"," 1.0209319591522217]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"6D3SuchUfyes","colab_type":"code","colab":{}},"source":["fC = pd.DataFrame([orDa[46],foCa[46]]).T\n","fC.columns = ['real_values', 'forecasts']\n","fC.to_csv('pv_forecasts.csv',sep = ',')"],"execution_count":0,"outputs":[]}]}