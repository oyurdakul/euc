{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Load.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1uEAMACimI9C2TkC63PVNUlEK6CtIFlgd","authorship_tag":"ABX9TyP4lkmhQU+rDBiJwnZBwVx2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TXxs8QpRVF7k","colab_type":"code","outputId":"c55d882c-0abb-4641-86a3-7396190f1a85","executionInfo":{"status":"error","timestamp":1581077618662,"user_tz":-60,"elapsed":4612,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","    TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","    When encoder is traine completely, merged model exhibits much better\n","    performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","    encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","    decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","%tensorflow_version 2.x\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","from tensorflow.keras import regularizers\n","\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","\n","import tensorflow as tf\n","\n","import matplotlib\n","\n","tf.test.gpu_device_name()\n","\n","with tf.device('/device:GPU:0'):\n","  # Different configurations for different computation structures\n","  # gpu = input('On Server?(y/n) : ')\n","  gpu = 'n'\n","  if(gpu == 'y'):\n","      # Ask for the parameters\n","      encoderEpoch    = 10\n","      decoderEpoch    = 5\n","      numNeuron       = 128\n","      batchSize       = 1\n","\n","      forecastHorizon = input('T - M         : ')\n","      directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = input('Data size     : ')\n","      M               = input('M             : ')\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","      stateful = True\n","\n","\n","      # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","  else:\n","      # Ask for the parameters\n","      encoderEpoch    = 50\n","      decoderEpoch    = 300\n","      numNeuron       = 512\n","      batchSize       = 1\n","\n","      forecastHorizon = '24'\n","      directoryName   = '/content/drive/My Drive/Colab Notebooks/load' # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = '1440'\n","      M               = '24'\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","\n","\n","      stateful = False\n","\n","\n","\n","      os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","  import matplotlib.pyplot as plt\n","\n","\n","\n","\n","  #################### Code starts\n","  # Parameters for LSTM input (we need to know the input shape)\n","  # These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","  num_months = 0\n","  num_days   = 0\n","  num_hours  = 0\n","\n","  def plot(figNum, originalData, lstmOuts, customTag=''):\n","      # lstmOuts -> predictions inverse transformed\n","      # originalData -> data read from file\n","\n","      # forecastLong : forecast with given (T - M)\n","      forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      listStart = -50\n","      listEnd   = -2\n","      plt.plot(originalData[listStart:listEnd])\n","      plt.plot(forecastLong[listStart:listEnd])\n","      plt.plot(persis[listStart+1:listEnd+1])\n","      plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","      title = ''\n","      title = title + customTag + '\\n'\n","      title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","      title = title + ('dataSize = ' + str(dataSize) + ', ')\n","      title = title + ('M = ' + str(M) + ', ')\n","      title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","      title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","      # title = title + ('loss = ' + str(loss) + ', ')\n","      # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","      title = title + ('(T - M) = ' + str(T-M) + ', ')\n","      title = title + ('currentEpoch = ' + str(figNum) + '')\n","      plt.title(title, fontsize=6)\n","\n","      plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","      plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","      plt.savefig(directoryName + '/' + customTag + str(figNum) + '.png', bbox_inches=\"tight\")\n","      plt.cla()\n","\n","  def plotTest(figNum, originalData, lstmOuts, customTag=''):\n","      # lstmOuts -> predictions inverse transformed\n","      # originalData -> data read from file\n","\n","      # forecastLong : forecast with given (T - M)\n","      forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      listStart = -50\n","      listEnd   = -2\n","      plt.plot(originalData[listStart:listEnd])\n","      plt.plot(forecastLong[listStart:listEnd])\n","      plt.plot(persis[listStart+1:listEnd+1])\n","      plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","      title = ''\n","      title = title + customTag + '\\n'\n","      title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","      title = title + ('dataSize = ' + str(dataSize) + ', ')\n","      title = title + ('M = ' + str(M) + ', ')\n","      title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","      title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","      # title = title + ('loss = ' + str(loss) + ', ')\n","      # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","      title = title + ('(T - M) = ' + str(T-M) + ', ')\n","      title = title + ('currentEpoch = ' + str(figNum) + '')\n","      plt.title(title, fontsize=6)\n","\n","      plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","      plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","      plt.savefig(directoryName + '/' + customTag + str(figNum) + '_test.png', bbox_inches=\"tight\")\n","      plt.cla()\n","\n","\n","  def noneNum(allorders):\n","      # Replace None's in a python list with 0's and return the output\n","      allorders = list(allorders)\n","      output = [i if i is not None else 0 for i in allorders]\n","\n","      return output\n","\n","  def calculateError(originalData, lstmOuts):\n","      forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      # Calculate error\n","      endIndex = -(T - M) + 1\n","      if endIndex == 0:\n","          endIndex = originalData.shape[0]\n","\n","      text = ''\n","\n","      maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MAE my model     : \" + str(maemym)\n","      text = text + '\\n'\n","      text = text + \"MAE persistence  : \" + str(maepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MSE my model     : \" + str(msemym)\n","      text = text + '\\n'\n","      text = text + \"MSE persistence  : \" + str(msepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      # # TODO - can be replaced by tensorflow\n","      # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # text = text + \"MAPE my model    : \" + str(mapemym)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","      \n","      # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # with tensorflow.Session() as sess:\n","          # mapemym_val = mapemym.eval()\n","          # mapepers_val = mapepers.eval()\n","      \n","      # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","\n","\n","      return text\n","\n","  #def mean_absolute_percentage_error(y_true, y_pred):\n","  #    # Not implemented in sklearn\n","  #\n","  #    # Not recommended to use since negative values\n","  #    # create negative losses\n","  #\n","  #    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","  def readData(dataType):\n","      if dataType == 'train':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_train.csv',sep=',')\n","      elif dataType == 'valid':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_valid.csv',sep=',')\n","      elif dataType == 'test':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_test.csv',sep=',')\n","\n","\n","      outputDat = np.array(data.total_consumption.iloc[:len(data)])\n","      date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","      return outputDat, date_time\n","\n","\n","  # Taken from test3.py\n","  # Taken from test3.py\n","  def prepareDataTraining(Xa, date_time):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","      \n","      dataSize = len(Xa)\n","\n","      scaler = MinMaxScaler(feature_range =(-1,1))\n","      X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","      \n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","      onehot_day_of_week   = OneHotEncoder(sparse=False)\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","      print(type(all_hours))\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","  # Taken from test3.py\n","  def prepareDataTesting(Xa, date_time,SCALER):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","\n","      dataSize = len(Xa)\n","      scaler = SCALER\n","      X = scaler.transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","      onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","  global forecasts\n","\n","  ############# Run code\n","  trainData, train_date_time = readData('train')\n","  testData,  test_date_time  = readData('valid')\n","  validData,  valid_date_time  = readData('test')\n","\n","  trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","  testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time, trainScaler)\n","  validScaler,  XvalidEncoder,  YvalidEncoder,  XvalidDecoder,  YvalidDecoder  = prepareDataTesting(validData , valid_date_time, trainScaler)\n","\n","\n","  ### Tensorboard modelling\n","  logdirEnc = \"load_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  logdirDec = \"/content/drive/My Drive/Colab Notebooks/load/decoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","  tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","  ### Create Encoder model\n","\n","  # Model layers\n","  num_features_encoder = 1 + (num_months + num_days + num_hours)\n","  inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","  encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  # encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, recurrent_dropout=0.4, kernel_regularizer=regularizers.l2(0.000001) ,stateful=stateful,activation = \"relu\")\n","  #encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  encoderDense = TimeDistributed(Dense(1))\n","\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, _ = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","  # encoderLSTM2Outputs      = encoderLSTM2(encoderLSTMOutputs)\n","  # encoderLSTM2Outputs, _, _= encoderLSTM2(encoderLSTMOutputs)\n","  encoderOutputs           = encoderDense(encoderLSTMOutputs)\n","\n","\n","  encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","  encoderModel.compile(optimizer='adam', loss='mse')\n","\n","  encoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/encoder-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n","  encoderCheckpoint = ModelCheckpoint(encoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","\n","  print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[encoderCheckpoint,tensorboardCallbackEncoder])\n","  # encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/encoder_load_e50.hdf5')\n","\n","\n","  ### Create merged (Encoder + Decoder) model\n","\n","  # Model layers\n","  num_features_decoder   = (num_months + num_days + num_hours)\n","  inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","  decoderBroadcasted     = Lambda(lambda x: broadcast_to(x, [batchSize, T-M, numNeuron]))\n","  decoderConcatenated    = Lambda(lambda x: concat(x, axis=2))\n","  decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","  decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","  # decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","  #decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","  decoderDense           = TimeDistributed(Dense(1))\n","  layerV                 = Dense(numNeuron)\n","  layerVActivation       = Activation('relu')\n","  layerV_prime           = Dense(numNeuron)\n","  layerV_primeActivation = Activation('relu')\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","  # Map encoder hidden state final to some vector(simple layer)\n","  c = layerVActivation(layerV(encoder_h))\n","  # Remap the c vector to h_prime\n","  h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","  # Use h_prime as input to other layers\n","  c_broadcasted              = decoderBroadcasted(c)\n","  decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","  # decoderStateZeros : just zeros to feed to the internal state\n","  decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","  decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","  # decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","  #decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","  decoderOutputs     = decoderDense(decoderLSTMOutputs)\n","\n","\n","  mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","  mergedModel.compile(optimizer='adam', loss='mse')\n","  print('Training merged(encoder + decoder) model...');\n","\n","  outputs = list()\n","\n","  \n","  \n","\n","  for i in range(decoderEpoch):\n","      decoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-\"+str(i)+\"-{val_loss:.2f}.hdf5\"\n","      decoderCheckpoint = ModelCheckpoint(decoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","      mergedModel.fit([XtrainEncoder, XtrainDecoder], [YtrainDecoder],epochs=1, shuffle=False, validation_data=([XtestEncoder, XtestDecoder], YtestDecoder),callbacks=[decoderCheckpoint,tensorboardCallbackDecoder])\n","\n","\n","      lstmOuts = mergedModel.predict([XtrainEncoder, XtrainDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = trainScaler.inverse_transform(lstmOuts)\n","      plot(i, trainData, lstmOuts, 'train')\n","      error_train = calculateError(trainData, lstmOuts)\n","\n","      lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = testScaler.inverse_transform(lstmOuts)\n","      plot(i, testData, lstmOuts, 'test')\n","      error_test = calculateError(testData, lstmOuts)\n","      outputs.append(lstmOuts[-1])\n","      outputsDF = pd.DataFrame(outputs)\n","      if i%10 == 0:\n","          outputsDF.to_csv(directoryName + '/' + 'output_' + str(i) + '.csv', sep = ',')\n","          outputs = list()\n","      lstmOuts = mergedModel.predict([XvalidEncoder, XvalidDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = validScaler.inverse_transform(lstmOuts)\n","      plotTest(i, validData, lstmOuts, 'test')\n","      \n","      print('train errors: ', error_train)\n","      print('test errors: ',error_test)\n","      \n","\n","  #np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-9823eaa49866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m   \u001b[0;31m# Model connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m   \u001b[0mencoderLSTMOutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputEncoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#encoders_c is already equal to encoderLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m   \u001b[0;31m# encoderLSTM2Outputs      = encoderLSTM2(encoderLSTMOutputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;31m# encoderLSTM2Outputs, _, _= encoderLSTM2(encoderLSTMOutputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2114\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   2299\u001b[0m         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m         caching_device=default_caching_device)\n\u001b[0m\u001b[1;32m   2302\u001b[0m     self.recurrent_kernel = self.add_weight(\n\u001b[1;32m   2303\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   def _variable_v2_call(cls,\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         shape=None):\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2594\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2597\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m     return variables.RefVariable(\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1409\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m   def _init_from_args(self,\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1540\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1542\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1544\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    121\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m    786\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     return op(\n\u001b[0;32m--> 788\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mminval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: /job:localhost/replica:0/task:0/device:GPU:0 unknown device."]}]},{"cell_type":"code","metadata":{"id":"s3_EzJBLVo9O","colab_type":"code","outputId":"6f9bf134-0c50-472b-9327-fa90801a2a54","executionInfo":{"status":"error","timestamp":1581281041542,"user_tz":-60,"elapsed":3031925,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","  TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","  When encoder is traine completely, merged model exhibits much better\n","  performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","  encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","  decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","%tensorflow_version 2.x\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","from tensorflow.keras import regularizers\n","\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","\n","import tensorflow as tf\n","\n","import matplotlib\n","\n","tf.test.gpu_device_name()\n","\n","with tf.device('/device:GPU:0'):\n","# Different configurations for different computation structures\n","# gpu = input('On Server?(y/n) : ')\n","  gpu = 'n'\n","  if(gpu == 'y'):\n","      # Ask for the parameters\n","      encoderEpoch    = 10\n","      decoderEpoch    = 5\n","      numNeuron       = 128\n","      batchSize       = 1\n","\n","      forecastHorizon = input('T - M         : ')\n","      directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = input('Data size     : ')\n","      M               = input('M             : ')\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","      stateful = True\n","\n","\n","      # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","  else:\n","      # Ask for the parameters\n","      encoderEpoch    = 50\n","      decoderEpoch    = 300\n","      numNeuron       = 512\n","      batchSize       = 1\n","\n","      forecastHorizon = '24'\n","      directoryName   = '/content/drive/My Drive/Colab Notebooks/load' # directory for the output plots (without slash, Example : test10-1)\n","      dataSize        = '1440'\n","      M               = '24'\n","\n","\n","      matplotlib.use('pdf')\n","\n","      forecastHorizon = int(forecastHorizon)\n","      M               = int(M)\n","      dataSize        = int(dataSize)\n","\n","      T = M + forecastHorizon\n","\n","\n","\n","      stateful = False\n","\n","\n","\n","      os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","  import matplotlib.pyplot as plt\n","\n","\n","\n","\n","  #################### Code starts\n","  # Parameters for LSTM input (we need to know the input shape)\n","  # These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","  num_months = 0\n","  num_days   = 0\n","  num_hours  = 0\n","\n","  def plot(figNum, originalData, lstmOuts, customTag=''):\n","      # lstmOuts -> predictions inverse transformed\n","      # originalData -> data read from file\n","\n","      # forecastLong : forecast with given (T - M)\n","      forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      listStart = -50\n","      listEnd   = -2\n","      plt.plot(originalData[listStart:listEnd])\n","      plt.plot(forecastLong[listStart:listEnd])\n","      plt.plot(persis[listStart+1:listEnd+1])\n","      plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","      title = ''\n","      title = title + customTag + '\\n'\n","      title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","      title = title + ('dataSize = ' + str(dataSize) + ', ')\n","      title = title + ('M = ' + str(M) + ', ')\n","      title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","      title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","      # title = title + ('loss = ' + str(loss) + ', ')\n","      # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","      title = title + ('(T - M) = ' + str(T-M) + ', ')\n","      title = title + ('currentEpoch = ' + str(figNum) + '')\n","      plt.title(title, fontsize=6)\n","\n","      plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","      plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","      plt.savefig(directoryName + '/' + customTag + str(figNum) + '.png', bbox_inches=\"tight\")\n","      plt.cla()\n","\n","  def plotTest(figNum, originalData, lstmOuts, customTag=''):\n","      # lstmOuts -> predictions inverse transformed\n","      # originalData -> data read from file\n","\n","      # forecastLong : forecast with given (T - M)\n","      forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      listStart = -50\n","      listEnd   = -2\n","      plt.plot(originalData[listStart:listEnd])\n","      plt.plot(forecastLong[listStart:listEnd])\n","      plt.plot(persis[listStart+1:listEnd+1])\n","      plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","      title = ''\n","      title = title + customTag + '\\n'\n","      title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","      title = title + ('dataSize = ' + str(dataSize) + ', ')\n","      title = title + ('M = ' + str(M) + ', ')\n","      title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","      title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","      # title = title + ('loss = ' + str(loss) + ', ')\n","      # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","      title = title + ('(T - M) = ' + str(T-M) + ', ')\n","      title = title + ('currentEpoch = ' + str(figNum) + '')\n","      plt.title(title, fontsize=6)\n","\n","      plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","      plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","      plt.savefig(directoryName + '/' + customTag + str(figNum) + '_test.png', bbox_inches=\"tight\")\n","      plt.cla()\n","\n","\n","  def noneNum(allorders):\n","      # Replace None's in a python list with 0's and return the output\n","      allorders = list(allorders)\n","      output = [i if i is not None else 0 for i in allorders]\n","\n","      return output\n","\n","  def calculateError(originalData, lstmOuts):\n","      forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","      startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","      forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","      startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","      persis         = np.concatenate([np.array([None]), originalData])\n","\n","      # Calculate error\n","      endIndex = -(T - M) + 1\n","      if endIndex == 0:\n","          endIndex = originalData.shape[0]\n","\n","      text = ''\n","\n","      maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MAE my model     : \" + str(maemym)\n","      text = text + '\\n'\n","      text = text + \"MAE persistence  : \" + str(maepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      text = text + \"MSE my model     : \" + str(msemym)\n","      text = text + '\\n'\n","      text = text + \"MSE persistence  : \" + str(msepers)\n","      text = text + '\\n'\n","      text = text + '\\n'\n","\n","      # # TODO - can be replaced by tensorflow\n","      # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # text = text + \"MAPE my model    : \" + str(mapemym)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","      \n","      # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","      # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","      # with tensorflow.Session() as sess:\n","          # mapemym_val = mapemym.eval()\n","          # mapepers_val = mapepers.eval()\n","      \n","      # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","      # text = text + '\\n'\n","      # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","      # text = text + '\\n'\n","      # text = text + '\\n'\n","\n","\n","      return text\n","\n","  #def mean_absolute_percentage_error(y_true, y_pred):\n","  #    # Not implemented in sklearn\n","  #\n","  #    # Not recommended to use since negative values\n","  #    # create negative losses\n","  #\n","  #    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","  def readData(dataType):\n","      if dataType == 'train':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_train.csv',sep=',')\n","      elif dataType == 'valid':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_valid.csv',sep=',')\n","      elif dataType == 'test':\n","          data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_test.csv',sep=',')\n","\n","\n","      outputDat = np.array(data.total_consumption.iloc[:len(data)])\n","      date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","      return outputDat, date_time\n","\n","\n","  # Taken from test3.py\n","  # Taken from test3.py\n","  def prepareDataTraining(Xa, date_time):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","      \n","      dataSize = len(Xa)\n","\n","      scaler = MinMaxScaler(feature_range =(-1,1))\n","      X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","      \n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","      onehot_day_of_week   = OneHotEncoder(sparse=False)\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","      print(type(all_hours))\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","  # Taken from test3.py\n","  def prepareDataTesting(Xa, date_time,SCALER):\n","      \"\"\"\n","      Data Representation :\n","      [samples, timesteps, features]\n","\n","      Each timestep :\n","      [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","      Example :\n","      [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","      Each sample :\n","      [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","      Overall(3D) :\n","      [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","      \"\"\"\n","\n","      dataSize = len(Xa)\n","      scaler = SCALER\n","      X = scaler.transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","  #    global num_hours\n","  #    global num_days\n","  #    global num_months\n","      \n","      # Prepare features\n","      all_hours  = [o.hour      for o in date_time]\n","      all_days   = [o.weekday() for o in date_time]\n","  #    all_months = [o.month     for o in date_time]\n","      \n","  #    num_hours = len(set(all_hours))\n","  #    num_days = len(set(all_days))\n","  #    num_months = len(set(all_months))\n","\n","      all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","      all_days   = np.array(all_days).reshape(len(all_days), 1)\n","  #    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","      global onehot_hour_of_day\n","      global onehot_day_of_week\n","  #    global onehot_month_of_year\n","      \n","      onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","      onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","  #    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","      encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","      encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","  #    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","      encoderX = []\n","      encoderY = []\n","      decoderX = []\n","      decoderY = []\n","\n","      # For later use, just skip...\n","      def convertList(inputNumber):\n","          return [inputNumber]\n","\n","      # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","      # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","      for i in range(dataSize - T):\n","          # -------------------- Samples\n","          customXsamples=[]\n","\n","          # j -> each look back timestep, index of the timesteps from 0 to M\n","          for j in range(T):\n","              # -------------------- Timesteps\n","              # Add each OneHotEncoding as a feature\n","              seriesOutputOneHot = []\n","              if j < M:\n","                  encoderMode = True\n","              else:\n","                  encoderMode = False\n","\n","              # All features are enabled\n","              hour_of_day_enabled   = True\n","              day_of_week_enabled   = True\n","  #            month_of_year_enabled = True\n","\n","              # -------------------- Features\n","              # Add features based on the flags\n","              global num_hours\n","              global num_days\n","  #            global num_months\n","              if hour_of_day_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                  num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","              if day_of_week_enabled:\n","                  # Add for the whole horizon\n","                  seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                  num_days = onehot_day_of_week.get_feature_names().shape[0]\n","  #\n","  #            if month_of_year_enabled:\n","  #                # Add for the whole horizon\n","  #                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","  #                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","              # Form input array\n","              add_to_input_array = []\n","              if encoderMode:\n","                  customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","              else:\n","                  customXsamples.append(seriesOutputOneHot)\n","              # Add features based on flags - DISABLE ALL : no past features are used in the article\n","              # if hour_of_day_enabled:\n","              #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","              # if day_of_week_enabled:\n","              #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","              # if month_of_year_enabled:\n","              #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","              # Add to encoder or decoder train data\n","              if M - 1 == j:\n","                  encoderX.append(customXsamples)\n","                  encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                  customXsamples = []\n","              elif T - 1 == j:\n","                  decoderX.append(customXsamples)\n","                  decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                  customXsamples = []\n","\n","\n","\n","      return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","  global forecasts\n","\n","  ############# Run code\n","  trainData, train_date_time = readData('train')\n","  testData,  test_date_time  = readData('valid')\n","  validData,  valid_date_time  = readData('test')\n","\n","  trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","  testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time, trainScaler)\n","  validScaler,  XvalidEncoder,  YvalidEncoder,  XvalidDecoder,  YvalidDecoder  = prepareDataTesting(validData , valid_date_time, trainScaler)\n","\n","\n","  ### Tensorboard modelling\n","  logdirEnc = \"load_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  logdirDec = \"/content/drive/My Drive/Colab Notebooks/load/decoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","  tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","  ### Create Encoder model\n","\n","  # Model layers\n","  num_features_encoder = 1 + (num_months + num_days + num_hours)\n","  inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","  encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  # encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, recurrent_dropout=0.4, kernel_regularizer=regularizers.l2(0.000001) ,stateful=stateful,activation = \"relu\")\n","  #encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","  encoderDense = TimeDistributed(Dense(1))\n","\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, _ = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","  # encoderLSTM2Outputs      = encoderLSTM2(encoderLSTMOutputs)\n","  # encoderLSTM2Outputs, _, _= encoderLSTM2(encoderLSTMOutputs)\n","  encoderOutputs           = encoderDense(encoderLSTMOutputs)\n","\n","\n","  encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","  encoderModel.compile(optimizer='adam', loss='mse')\n","\n","  encoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/encoder-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n","  encoderCheckpoint = ModelCheckpoint(encoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","\n","  #print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[encoderCheckpoint,tensorboardCallbackEncoder])\n","  encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/load_old/models/encoder-weights-improvement-12-0.07.hdf5')\n","\n","\n","  ### Create merged (Encoder + Decoder) model\n","\n","  # Model layers\n","  num_features_decoder   = (num_months + num_days + num_hours)\n","  inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","  decoderBroadcasted     = Lambda(lambda x: broadcast_to(x, [batchSize, T-M, numNeuron]))\n","  decoderConcatenated    = Lambda(lambda x: concat(x, axis=2))\n","  decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","  decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","  decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","  #decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","  decoderDense           = TimeDistributed(Dense(1))\n","  layerV                 = Dense(numNeuron)\n","  layerVActivation       = Activation('relu')\n","  layerV_prime           = Dense(numNeuron)\n","  layerV_primeActivation = Activation('relu')\n","\n","  # Model connections\n","  encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","  # Map encoder hidden state final to some vector(simple layer)\n","  c = layerVActivation(layerV(encoder_h))\n","  # Remap the c vector to h_prime\n","  h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","  # Use h_prime as input to other layers\n","  c_broadcasted              = decoderBroadcasted(c)\n","  decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","  # decoderStateZeros : just zeros to feed to the internal state\n","  decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","  decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","  decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","  #decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","  decoderOutputs     = decoderDense(decoderLSTMOutputs2)\n","\n","\n","  mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","  mergedModel.compile(optimizer='adam', loss='mse')\n","  print('Training merged(encoder + decoder) model...');\n","\n","  outputs = list()\n","  mergedModel = load_model('/content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-49-0.11.hdf5')\n","\n","\n","\n","  for i in range(50, decoderEpoch):\n","      decoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-\"+str(i)+\"-{val_loss:.2f}.hdf5\"\n","      decoderCheckpoint = ModelCheckpoint(decoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","      mergedModel.fit([XtrainEncoder, XtrainDecoder], [YtrainDecoder],epochs=1, shuffle=False, validation_data=([XtestEncoder, XtestDecoder], YtestDecoder),callbacks=[decoderCheckpoint,tensorboardCallbackDecoder])\n","\n","\n","      lstmOuts = mergedModel.predict([XtrainEncoder, XtrainDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = trainScaler.inverse_transform(lstmOuts)\n","      plot(i, trainData, lstmOuts, 'train')\n","      error_train = calculateError(trainData, lstmOuts)\n","\n","      lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = testScaler.inverse_transform(lstmOuts)\n","      plot(i, testData, lstmOuts, 'test')\n","      error_test = calculateError(testData, lstmOuts)\n","      outputs.append(lstmOuts[-1])\n","      outputsDF = pd.DataFrame(outputs)\n","      if i%10 == 0:\n","          outputsDF.to_csv(directoryName + '/' + 'output_' + str(i) + '.csv', sep = ',')\n","          outputs = list()\n","      lstmOuts = mergedModel.predict([XvalidEncoder, XvalidDecoder])\n","      lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","      lstmOuts = validScaler.inverse_transform(lstmOuts)\n","      plotTest(i, validData, lstmOuts, 'test')\n","      \n","      print('train errors: ', error_train)\n","      print('test errors: ',error_test)\n","      \n","\n","#np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","Training merged(encoder + decoder) model...\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0081\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-50-0.13.hdf5\n","1248/1248 [==============================] - 79s 63ms/sample - loss: 0.0081 - val_loss: 0.1302\n","train errors:  MAE my model     : 3.2838093596672993\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.548924574493164\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.891441972172725\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 32.00149426806342\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0055\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-51-0.13.hdf5\n","1248/1248 [==============================] - 78s 63ms/sample - loss: 0.0055 - val_loss: 0.1306\n","train errors:  MAE my model     : 3.343304434535788\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.58624512897024\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.985327634558739\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 33.20947328286986\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0050\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-52-0.13.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0050 - val_loss: 0.1305\n","train errors:  MAE my model     : 3.283310938112062\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.747468858031574\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.879563459589575\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 31.55432967079216\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0053\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-53-0.13.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0053 - val_loss: 0.1293\n","train errors:  MAE my model     : 3.2320196858658803\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 18.89821679249385\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.886537386728262\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 30.382541948042547\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0045\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-54-0.14.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0045 - val_loss: 0.1358\n","train errors:  MAE my model     : 3.343094077447016\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.746343395581604\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.050746798547522\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 32.54283732353525\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0049\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-55-0.12.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0049 - val_loss: 0.1246\n","train errors:  MAE my model     : 3.2362190087458154\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.09529857469468\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.914689202256636\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 31.45990415884156\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0055\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-56-0.14.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0055 - val_loss: 0.1361\n","train errors:  MAE my model     : 3.36623045590403\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.84469249575231\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.051765694001433\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 34.00507094817022\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0064\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-57-0.13.hdf5\n","1248/1248 [==============================] - 88s 70ms/sample - loss: 0.0064 - val_loss: 0.1276\n","train errors:  MAE my model     : 3.2072799716305025\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 19.943448175258805\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.885601845728886\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 31.730736940179202\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0060\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-58-0.14.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0060 - val_loss: 0.1373\n","train errors:  MAE my model     : 3.3602203254608085\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.557400139164265\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.095530198540626\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 34.217635391377826\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0065\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-59-0.09.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0065 - val_loss: 0.0948\n","train errors:  MAE my model     : 3.355013575014064\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 18.770918553738074\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.483657679213487\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 22.318625105596016\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0062\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-60-0.15.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0062 - val_loss: 0.1479\n","train errors:  MAE my model     : 3.2013340553271474\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 19.493204403854683\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.041510474148044\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 35.2663454742862\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0055\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-61-0.12.hdf5\n","1248/1248 [==============================] - 86s 69ms/sample - loss: 0.0055 - val_loss: 0.1227\n","train errors:  MAE my model     : 3.365544111622917\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.69806540020814\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.8898577173703677\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 29.533794320408777\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0056\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-62-0.13.hdf5\n","1248/1248 [==============================] - 87s 70ms/sample - loss: 0.0056 - val_loss: 0.1327\n","train errors:  MAE my model     : 3.276015714842382\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.34806869318403\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.685357286061869\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 27.338131644647195\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0076\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-63-0.13.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0076 - val_loss: 0.1322\n","train errors:  MAE my model     : 3.377129623361165\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.923774663632773\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.0295913524875395\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 33.34027261340761\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0063\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-64-0.14.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0063 - val_loss: 0.1405\n","train errors:  MAE my model     : 3.251005503920959\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.849837915837654\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.091926689217308\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 34.26375788496186\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0051\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-65-0.12.hdf5\n","1248/1248 [==============================] - 86s 69ms/sample - loss: 0.0051 - val_loss: 0.1244\n","train errors:  MAE my model     : 3.2049968095234247\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 19.555612516711292\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.833347811968915\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 30.707077590856876\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0056\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-66-0.14.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0056 - val_loss: 0.1353\n","train errors:  MAE my model     : 3.2722185755073783\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.922441414866793\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.9847657483683\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 32.69355095456967\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0058\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-67-0.14.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0058 - val_loss: 0.1425\n","train errors:  MAE my model     : 3.1482176110578406\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 18.92569024487303\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.119479041805515\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 35.504396631144644\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0066\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-68-0.13.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 0.0066 - val_loss: 0.1295\n","train errors:  MAE my model     : 3.1958811568297607\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 19.953460518686963\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.8729606651504316\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 31.22834182950751\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0057\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-69-0.15.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0057 - val_loss: 0.1488\n","train errors:  MAE my model     : 3.2061444669743366\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 19.512849579273936\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.033947668516481\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 34.30998498351517\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0053\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-70-0.14.hdf5\n","1248/1248 [==============================] - 77s 61ms/sample - loss: 0.0053 - val_loss: 0.1378\n","train errors:  MAE my model     : 3.3048073977400914\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.250286508927697\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.034025645654851\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 32.9697615694997\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0046\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-71-0.13.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0046 - val_loss: 0.1338\n","train errors:  MAE my model     : 3.2699285673533947\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.249941779206985\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.956206021831562\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 32.73524716577289\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0050\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-72-0.14.hdf5\n","1248/1248 [==============================] - 79s 63ms/sample - loss: 0.0050 - val_loss: 0.1364\n","train errors:  MAE my model     : 3.261999434392485\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.105869134312393\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.009827217429025\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 33.15398553419104\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0052\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-73-0.13.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0052 - val_loss: 0.1285\n","train errors:  MAE my model     : 3.170010193933956\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 19.063511608700427\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.8626391118631735\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 31.30461090641567\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0046\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-74-0.13.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0046 - val_loss: 0.1283\n","train errors:  MAE my model     : 3.2915383037264774\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 20.485656150790803\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.8475328458117204\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 29.623171710300387\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 0.0046\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-75-0.14.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 0.0046 - val_loss: 0.1395\n","train errors:  MAE my model     : 3.346568129598283\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 21.433144897508996\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.057336653632622\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 34.09958079884839\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 385303.5649\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-76-770097.17.hdf5\n","1248/1248 [==============================] - 77s 62ms/sample - loss: 384994.8281 - val_loss: 770097.1741\n","train errors:  MAE my model     : 2.976804912405266\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 16.93721636095444\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 3.702670696526069\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 26.903110374517677\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: 748494673.8301\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-77-2600205.93.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: 747894918.4825 - val_loss: 2600205.9341\n","train errors:  MAE my model     : 3.102582596820865\n","MAE persistence  : 2.519923939151321\n","\n","MSE my model     : 16.28447834403519\n","MSE persistence  : 12.194020664631706\n","\n","\n","test errors:  MAE my model     : 4.055136419519202\n","MAE persistence  : 2.6011974025974025\n","\n","MSE my model     : 30.355448158150168\n","MSE persistence  : 12.116096900324676\n","\n","\n","Train on 1248 samples, validate on 384 samples\n","1247/1248 [============================>.] - ETA: 0s - loss: nan\n","Epoch 00001: saving model to /content/drive/My Drive/Colab Notebooks/load/models/decoder-weights-improvement-78-nan.hdf5\n","1248/1248 [==============================] - 78s 62ms/sample - loss: nan - val_loss: nan\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-af070a4b2fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mlstmOuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstmOuts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmOuts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstmOuts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m       \u001b[0mlstmOuts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmOuts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m       \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstmOuts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m       \u001b[0merror_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstmOuts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-af070a4b2fa5>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(figNum, originalData, lstmOuts, customTag)\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginalData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstmOuts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Original Data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Persistance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'First Forecast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectoryName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustomTag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigNum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-af070a4b2fa5>\u001b[0m in \u001b[0;36mcalculateError\u001b[0;34m(originalData, lstmOuts)\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m       \u001b[0mmaemym\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoneNum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecastLong\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moriginalData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mendIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m       \u001b[0mmaepers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoneNum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginalData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mendIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"MAE my model     : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaemym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \"\"\"\n\u001b[1;32m    177\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 178\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m     84\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."]}]},{"cell_type":"code","metadata":{"id":"bZBkT9rrVuoQ","colab_type":"code","outputId":"1d2ebc2f-48e2-40f3-d750-c32292122663","executionInfo":{"status":"ok","timestamp":1581263372066,"user_tz":-60,"elapsed":9734,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XvrjwHp8W58F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"0b05412a-58d3-4c4f-ce85-6143c0bd4f06","executionInfo":{"status":"ok","timestamp":1581327130152,"user_tz":-60,"elapsed":27234,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","    TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","    When encoder is traine completely, merged model exhibits much better\n","    performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","    encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","    decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","\n","import tensorflow as tf\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","import matplotlib\n","\n","# Different configurations for different computation structures\n","# gpu = input('On Server?(y/n) : ')\n","gpu = 'n'\n","if(gpu == 'y'):\n","    # Ask for the parameters\n","    encoderEpoch    = 10\n","    decoderEpoch    = 5\n","    numNeuron       = 128\n","    batchSize       = 1\n","\n","    forecastHorizon = input('T - M         : ')\n","    directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = input('Data size     : ')\n","    M               = input('M             : ')\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","    stateful = True\n","\n","\n","    # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","else:\n","    # Ask for the parameters\n","    encoderEpoch    = 100\n","    decoderEpoch    = 300\n","    numNeuron       = 512\n","    batchSize       = 1\n","\n","    forecastHorizon = '24'\n","    directoryName   = '/content/drive/My Drive/Colab Notebooks' # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = '1440'\n","    M               = '24'\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","\n","\n","    stateful = False\n","\n","\n","\n","    os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","\n","#################### Code starts\n","# Parameters for LSTM input (we need to know the input shape)\n","# These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","num_months = 0\n","num_days   = 0\n","num_hours  = 0\n","\n","def plot(figNum, originalData, lstmOuts, customTag=''):\n","    # lstmOuts -> predictions inverse transformed\n","    # originalData -> data read from file\n","\n","    # forecastLong : forecast with given (T - M)\n","    forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    listStart = -25\n","    listEnd   = -1\n","    plt.plot(originalData[listStart:listEnd])\n","    plt.plot(forecastLong[listStart:listEnd])\n","#    plt.plot(persis[listStart+1:listEnd+1])\n","#    plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","    title = ''\n","    title = title + customTag + '\\n'\n","    title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","    title = title + ('dataSize = ' + str(dataSize) + ', ')\n","    title = title + ('M = ' + str(M) + ', ')\n","    title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","    title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","    # title = title + ('loss = ' + str(loss) + ', ')\n","    # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","    title = title + ('(T - M) = ' + str(T-M) + ', ')\n","    title = title + ('currentEpoch = ' + str(figNum) + '')\n","    plt.title(title, fontsize=6)\n","\n","#    plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","    plt.legend(['Original Data', 'LSTM'])\n","    plt.savefig(directoryName + '/' + customTag + str(figNum) + 'test.png', bbox_inches=\"tight\")\n","    plt.cla()\n","    return forecastLong[listStart:listEnd],originalData[listStart:listEnd]\n","\n","\n","def noneNum(allorders):\n","    # Replace None's in a python list with 0's and return the output\n","    allorders = list(allorders)\n","    output = [i if i is not None else 0 for i in allorders]\n","\n","    return output\n","\n","def calculateError(originalData, lstmOuts):\n","    forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    # Calculate error\n","    endIndex = -(T - M) + 1\n","    if endIndex == 0:\n","        endIndex = originalData.shape[0]\n","\n","    text = ''\n","\n","    maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MAE my model     : \" + str(maemym)\n","    text = text + '\\n'\n","    text = text + \"MAE persistence  : \" + str(maepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MSE my model     : \" + str(msemym)\n","    text = text + '\\n'\n","    text = text + \"MSE persistence  : \" + str(msepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    # # TODO - can be replaced by tensorflow\n","    # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # text = text + \"MAPE my model    : \" + str(mapemym)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","    \n","    # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # with tensorflow.Session() as sess:\n","        # mapemym_val = mapemym.eval()\n","        # mapepers_val = mapepers.eval()\n","    \n","    # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","\n","\n","    return text\n","\n","#def mean_absolute_percentage_error(y_true, y_pred):\n","#    # Not implemented in sklearn\n","#\n","#    # Not recommended to use since negative values\n","#    # create negative losses\n","#\n","#    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","def readData(dataType):\n","    if dataType == 'train':\n","        data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_train.csv',sep=',')\n","    elif dataType == 'test':\n","        data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_test.csv',sep=',')\n","\n","    outputDat = np.array(data.total_consumption.iloc[:len(data)])\n","    date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","    return outputDat, date_time\n","\n","\n","# Taken from test3.py\n","# Taken from test3.py\n","def prepareDataTraining(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","    \n","    dataSize = len(Xa)\n","\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","    \n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","    onehot_day_of_week   = OneHotEncoder(sparse=False)\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","    print(type(all_hours))\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","# Taken from test3.py\n","def prepareDataTesting(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","\n","    dataSize = len(Xa)\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","    onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","global forecasts\n","\n","############# Run code\n","trainData, train_date_time = readData('train')\n","testData,  test_date_time  = readData('test')\n","\n","trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time)\n","\n","\n","### Tensorboard modelling\n","#logdirEnc = \"pv_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","logdirDec = \"pv_final\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","### Create Encoder model\n","\n","# # Model layers\n","# num_features_encoder = 1 + (num_months + num_days + num_hours)\n","# inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","# encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful, activation = \"relu\")\n","# #encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","# #encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","# encoderDense = TimeDistributed(Dense(1))\n","\n","\n","# # Model connections\n","# encoderLSTMOutputs, _, _  = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","# #encoderLSTM2Outputs      = encoderLSTM2(encoderLSTMOutputs)\n","# #encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\n","# encoderOutputs           = encoderDense(encoderLSTMOutputs)\n","\n","\n","# encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","# encoderModel.compile(optimizer='adam', loss='mse')\n","#print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,verbose = 0,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[tensorboardCallbackEncoder])\n","encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/load_encoder.hdf5')\n","\n","\n","# ### Create merged (Encoder + Decoder) model\n","\n","# # Model layers\n","# num_features_decoder   = (num_months + num_days + num_hours)\n","# inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","# decoderBroadcasted     = Lambda(lambda x: tf.broadcast_to(x, [batchSize, T-M, numNeuron]))\n","# decoderConcatenated    = Lambda(lambda x: tf.concat(x, axis=2))\n","# decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","# decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","# decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","# #decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","# decoderDense           = TimeDistributed(Dense(1))\n","# layerV                 = Dense(numNeuron)\n","# layerVActivation       = Activation('relu')\n","# layerV_prime           = Dense(numNeuron)\n","# layerV_primeActivation = Activation('relu')\n","\n","# # Model connections\n","# encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","# # Map encoder hidden state final to some vector(simple layer)\n","# c = layerVActivation(layerV(encoder_h))\n","# # Remap the c vector to h_prime\n","# h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","# # Use h_prime as input to other layers\n","# c_broadcasted              = decoderBroadcasted(c)\n","# decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","# # decoderStateZeros : just zeros to feed to the internal state\n","# decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","# decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","# decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","# #decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","# decoderOutputs     = decoderDense(decoderLSTMOutputs2)\n","\n","i = 67\n","# mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","# mergedModel.compile(optimizer='adam', loss='mse')\n","print('Training merged(encoder + decoder) model...');\n","mergedModel = load_model('/content/drive/My Drive/Colab Notebooks/load_decoder.hdf5')\n","#outputs = list()\n","# mypath = 'load_decoder/'\n","# modelList = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","foCa = list()\n","orDa = list()\n","#    mergedModel = load_model('decoder_load_e86.hdf5')\n","lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","#lstmOuts = mergedModel.predict([XtrainEncoder, XtrainDecoder])\n","lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","#    Tscaler = MinMaxScaler(feature_range =(-1,1))\n","#    lstmOuts = lstmOuts.reshape(lstmOuts.shape[0], lstmOuts.shape[1])\n","lstmOuts = trainScaler.inverse_transform(lstmOuts)\n","fc, od = plot(i, testData, lstmOuts, 'test')\n","#fc, od = plot(i, trainData, lstmOuts, 'test')\n","\n","foCa.append(fc)\n","orDa.append(od)\n","\n","    \n","    #error_test = calculateError(testData, lstmOuts)\n","    #outputs.append(lstmOuts[-1])\n","    #outputsDF = pd.DataFrame(outputs)\n","    #if i%10 == 0:\n","    #    outputsDF.to_csv(directoryName + '/' + 'output_' + str(i) + '.csv', sep = ',')\n","    #    outputs = list()\n","    \n","#    print('train errors: ', error_train)\n","#    print('test errors: ',error_test)\n","        \n","#np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","Training merged(encoder + decoder) model...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KiawJHXrcaDB","colab_type":"code","colab":{}},"source":["fC = pd.DataFrame([od,fc]).T\n","fC.columns = ['real_values', 'forecasts']\n","fC.to_csv('load_forecasts.csv',sep = ',')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYQoIdOW-yW5","colab_type":"code","outputId":"eb78ff0a-4c6a-4c7b-8844-3e9173be9790","executionInfo":{"status":"ok","timestamp":1581076682497,"user_tz":-60,"elapsed":482,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jan  7 15:07:54 2020\n","\n","@author: yubim\n","\"\"\"\n","\n","# Functional API test file\n","\n","\n","'''\n","Notes :\n","  TimeDistributed : create model that process sequence of inputs\n","\n","'''\n","\n","'''\n","Additional notes :\n","  When encoder is traine completely, merged model exhibits much better\n","  performance rather than training decoder with more epochs.\n","'''\n","\n","\n","'''\n","Variables :\n","  encoder_h : h^<N>, encoder hidden state(maps to internal state of LSTM, C)\n","  decoder_h : decoder hidden state(maps to internal state of LSTM, C)\n","\n","'''\n","%tensorflow_version 2.x\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import LSTM, Input, Dense, TimeDistributed, Lambda, Activation, Dropout\n","\n","import numpy as np\n","import pandas as pd\n","\n","import datetime\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tensorflow.keras.metrics import mean_absolute_percentage_error\n","\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","\n","# Parameters\n","from tensorflow import concat\n","from tensorflow import broadcast_to\n","\n","\n","import tensorflow as tf\n","\n","import matplotlib\n","\n","# tf.test.gpu_device_name()\n","\n","# with tf.device('/device:GPU:0'):\n","# Different configurations for different computation structures\n","# gpu = input('On Server?(y/n) : ')\n","gpu = 'n'\n","if(gpu == 'y'):\n","    # Ask for the parameters\n","    encoderEpoch    = 10\n","    decoderEpoch    = 5\n","    numNeuron       = 128\n","    batchSize       = 1\n","\n","    forecastHorizon = input('T - M         : ')\n","    directoryName   = input('Directory     : ') # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = input('Data size     : ')\n","    M               = input('M             : ')\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","    stateful = True\n","\n","\n","    # sys.stdout = open(directoryName + '/' + 'log.txt', \"w\")\n","else:\n","    # Ask for the parameters\n","    encoderEpoch    = 50\n","    decoderEpoch    = 300\n","    numNeuron       = 512\n","    batchSize       = 1\n","\n","    forecastHorizon = '24'\n","    directoryName   = '/content/drive/My Drive/Colab Notebooks/load' # directory for the output plots (without slash, Example : test10-1)\n","    dataSize        = '1440'\n","    M               = '24'\n","\n","\n","    matplotlib.use('pdf')\n","\n","    forecastHorizon = int(forecastHorizon)\n","    M               = int(M)\n","    dataSize        = int(dataSize)\n","\n","    T = M + forecastHorizon\n","\n","\n","\n","    stateful = False\n","\n","\n","\n","    os.environ['CUDA_VISIBLE_DEVICES']=\"-1\"\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","\n","#################### Code starts\n","# Parameters for LSTM input (we need to know the input shape)\n","# These are dynamic (2 month will not yield OneHotEncoding with 12 length)\n","num_months = 0\n","num_days   = 0\n","num_hours  = 0\n","\n","def plot(figNum, originalData, lstmOuts, customTag=''):\n","    # lstmOuts -> predictions inverse transformed\n","    # originalData -> data read from file\n","\n","    # forecastLong : forecast with given (T - M)\n","    forecastLong   = [lstmOuts[i//(T-M)*(T-M)][i%(T-M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    listStart = -50\n","    listEnd   = -2\n","    plt.plot(originalData[listStart:listEnd])\n","    plt.plot(forecastLong[listStart:listEnd])\n","    plt.plot(persis[listStart+1:listEnd+1])\n","    plt.plot(startingPoints[listStart:listEnd], 'rX')\n","\n","    title = ''\n","    title = title + customTag + '\\n'\n","    title = title + ('numNeuron = ' + str(numNeuron) + ', ')\n","    title = title + ('dataSize = ' + str(dataSize) + ', ')\n","    title = title + ('M = ' + str(M) + ', ')\n","    title = title + ('encoderEpoch = ' + str(encoderEpoch) + ', ')\n","    title = title + ('decoderEpoch = ' + str(decoderEpoch) + ', \\n')\n","    # title = title + ('loss = ' + str(loss) + ', ')\n","    # title = title + ('optimizer = ' + str(optimizer) + ', ')\n","    title = title + ('(T - M) = ' + str(T-M) + ', ')\n","    title = title + ('currentEpoch = ' + str(figNum) + '')\n","    plt.title(title, fontsize=6)\n","\n","    plt.text(-0.1, 0, calculateError(originalData, lstmOuts), fontsize=6)\n","    plt.legend(['Original Data', 'LSTM', 'Persistance', 'First Forecast'])\n","    plt.savefig(directoryName + '/' + customTag + str(figNum) + '.png', bbox_inches=\"tight\")\n","    plt.cla()\n","\n","\n","def noneNum(allorders):\n","    # Replace None's in a python list with 0's and return the output\n","    allorders = list(allorders)\n","    output = [i if i is not None else 0 for i in allorders]\n","\n","    return output\n","\n","def calculateError(originalData, lstmOuts):\n","    forecastLong   = [lstmOuts[i//(T - M)*(T - M)][i%(T - M)] for i in range(lstmOuts.shape[0])]\n","    startingPoints = [lstmOuts[i][0] for i in range(lstmOuts.shape[0])]\n","\n","    forecastLong   = np.concatenate([np.array([None]*(M+1)), forecastLong])\n","    startingPoints = np.concatenate([np.array([None]*(M+1)), startingPoints])\n","    persis         = np.concatenate([np.array([None]), originalData])\n","\n","    # Calculate error\n","    endIndex = -(T - M) + 1\n","    if endIndex == 0:\n","        endIndex = originalData.shape[0]\n","\n","    text = ''\n","\n","    maemym  = mean_absolute_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    maepers = mean_absolute_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MAE my model     : \" + str(maemym)\n","    text = text + '\\n'\n","    text = text + \"MAE persistence  : \" + str(maepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    msemym  = mean_squared_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    msepers = mean_squared_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    text = text + \"MSE my model     : \" + str(msemym)\n","    text = text + '\\n'\n","    text = text + \"MSE persistence  : \" + str(msepers)\n","    text = text + '\\n'\n","    text = text + '\\n'\n","\n","    # # TODO - can be replaced by tensorflow\n","    # mapemym  = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # text = text + \"MAPE my model    : \" + str(mapemym)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","    \n","    # mapemym = mean_absolute_percentage_error(noneNum(forecastLong[M:]) , originalData[M:endIndex])\n","    # mapepers = mean_absolute_percentage_error(noneNum(persis[M:-(T - M)]), originalData[M:endIndex])\n","    # with tensorflow.Session() as sess:\n","        # mapemym_val = mapemym.eval()\n","        # mapepers_val = mapepers.eval()\n","    \n","    # text = text + \"MAPE my model    : \" + str(mapemym_val)\n","    # text = text + '\\n'\n","    # text = text + \"MAPE persistence : \" + str(mapepers_val)\n","    # text = text + '\\n'\n","    # text = text + '\\n'\n","\n","\n","    return text\n","\n","#def mean_absolute_percentage_error(y_true, y_pred):\n","#    # Not implemented in sklearn\n","#\n","#    # Not recommended to use since negative values\n","#    # create negative losses\n","#\n","#    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","def readData(dataType):\n","    # if dataType == 'train':\n","    #     data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_train.csv',sep=',')\n","    # elif dataType == 'valid':\n","    data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/load_valid.csv',sep=',')\n","\n","    outputDat = np.array(data.total_consumption.iloc[:len(data)])\n","    date_time = [datetime.datetime.strptime(data.timestamp.iloc[i], '%Y-%m-%d %H:%M:%S') for i in range(1,len(data))]\n","\n","    return outputDat, date_time\n","\n","\n","# Taken from test3.py\n","# Taken from test3.py\n","def prepareDataTraining(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","    \n","    dataSize = len(Xa)\n","\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","    \n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False)\n","    onehot_day_of_week   = OneHotEncoder(sparse=False)\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","    print(type(all_hours))\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","# Taken from test3.py\n","def prepareDataTesting(Xa, date_time):\n","    \"\"\"\n","    Data Representation :\n","    [samples, timesteps, features]\n","\n","    Each timestep :\n","    [consumptionValue, 24 vector of One Hot Encoding of Input, forecast * 24 vector of One Hot Encoding of Output]\n","    Example :\n","    [0.13, 0, 0, 1, 0, ..., 0, | 0, 1, ..., 0, | 0, 0, 0, 1, .., 0 ] -> 1 + 24 + np.rizon * 24 elements (| is used as a seperator to make it readeable)\n","\n","    Each sample :\n","    [ timestep_0, timestep_1, ..., timestep_n ]\n","\n","    Overall(3D) :\n","    [ [ timestep_0, timestep_1, ..., timestep_n ], [ timestep_0, timestep_1, ..., timestep_n ] ... ]\n","    \"\"\"\n","\n","    dataSize = len(Xa)\n","    scaler = MinMaxScaler(feature_range =(-1,1))\n","    X = scaler.fit_transform(Xa.reshape(Xa.shape[0],1)).reshape(Xa.shape[0])\n","\n","#    global num_hours\n","#    global num_days\n","#    global num_months\n","    \n","    # Prepare features\n","    all_hours  = [o.hour      for o in date_time]\n","    all_days   = [o.weekday() for o in date_time]\n","#    all_months = [o.month     for o in date_time]\n","    \n","#    num_hours = len(set(all_hours))\n","#    num_days = len(set(all_days))\n","#    num_months = len(set(all_months))\n","\n","    all_hours  = np.array(all_hours).reshape(len(all_hours), 1)\n","    all_days   = np.array(all_days).reshape(len(all_days), 1)\n","#    all_months = np.array(all_months).reshape(len(all_months), 1)\n","\n","    global onehot_hour_of_day\n","    global onehot_day_of_week\n","#    global onehot_month_of_year\n","    \n","    onehot_hour_of_day   = OneHotEncoder(sparse=False, categories='auto')\n","    onehot_day_of_week   = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(sparse=False, categories='auto')\n","#    onehot_month_of_year = OneHotEncoder(n_values = 13, sparse=False)\n","\n","    encoded_hour_of_day   = onehot_hour_of_day.fit_transform(all_hours)\n","    encoded_day_of_week   = onehot_day_of_week.fit_transform(all_days)\n","#    encoded_month_of_year = onehot_month_of_year.fit_transform(all_months)\n","\n","    encoderX = []\n","    encoderY = []\n","    decoderX = []\n","    decoderY = []\n","\n","    # For later use, just skip...\n","    def convertList(inputNumber):\n","        return [inputNumber]\n","\n","    # i -> each sample, index of the samples from 0 to (ds - M - (T - M) + 1) = (ds - T + 1)\n","    # TODO - sanki + 1 olmamali asagida??? -> olunca disina tasti zaten, bir gariplik var ama cozemedim, eski kod ile farki ne ki buranin?\n","    for i in range(dataSize - T):\n","        # -------------------- Samples\n","        customXsamples=[]\n","\n","        # j -> each look back timestep, index of the timesteps from 0 to M\n","        for j in range(T):\n","            # -------------------- Timesteps\n","            # Add each OneHotEncoding as a feature\n","            seriesOutputOneHot = []\n","            if j < M:\n","                encoderMode = True\n","            else:\n","                encoderMode = False\n","\n","            # All features are enabled\n","            hour_of_day_enabled   = True\n","            day_of_week_enabled   = True\n","#            month_of_year_enabled = True\n","\n","            # -------------------- Features\n","            # Add features based on the flags\n","            global num_hours\n","            global num_days\n","#            global num_months\n","            if hour_of_day_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_hour_of_day[i+j])\n","                num_hours = onehot_hour_of_day.get_feature_names().shape[0]\n","\n","            if day_of_week_enabled:\n","                # Add for the whole horizon\n","                seriesOutputOneHot.extend(encoded_day_of_week[i+j])\n","                num_days = onehot_day_of_week.get_feature_names().shape[0]\n","#\n","#            if month_of_year_enabled:\n","#                # Add for the whole horizon\n","#                seriesOutputOneHot.extend(encoded_month_of_year[i+j])\n","#                num_months = onehot_month_of_year.get_feature_names().shape[0]\n","\n","\n","\n","            # Form input array\n","            add_to_input_array = []\n","            if encoderMode:\n","                customXsamples.append([X[i+j]] + seriesOutputOneHot)\n","            else:\n","                customXsamples.append(seriesOutputOneHot)\n","            # Add features based on flags - DISABLE ALL : no past features are used in the article\n","            # if hour_of_day_enabled:\n","            #     add_to_input_array.extend(list(encoded_hour_of_day[i+j]))\n","            # if day_of_week_enabled:\n","            #     add_to_input_array.extend(list(encoded_day_of_week[i+j]))\n","            # if month_of_year_enabled:\n","            #     add_to_input_array.extend(list(encoded_month_of_year[i+j]))\n","\n","\n","            # Add to encoder or decoder train data\n","            if M - 1 == j:\n","                encoderX.append(customXsamples)\n","                encoderY.append(list(map(convertList, X[i + 1 : i + M + 1])))\n","                customXsamples = []\n","            elif T - 1 == j:\n","                decoderX.append(customXsamples)\n","                decoderY.append(list(map(convertList, X[i + M + 1 : i + T + 1])))\n","                customXsamples = []\n","\n","\n","\n","    return scaler, np.array(encoderX), np.array(encoderY), np.array(decoderX), np.array(decoderY)\n","\n","\n","global forecasts\n","\n","############# Run code\n","# trainData, train_date_time = readData('train')\n","testData,  test_date_time  = readData('valid')\n","\n","# trainScaler, XtrainEncoder, YtrainEncoder, XtrainDecoder, YtrainDecoder = prepareDataTraining(trainData, train_date_time)\n","testScaler,  XtestEncoder,  YtestEncoder,  XtestDecoder,  YtestDecoder  = prepareDataTesting(testData , test_date_time)\n","\n","\n","\n","### Tensorboard modelling\n","# logdirEnc = \"load_final/encoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# logdirDec = \"/content/drive/My Drive/Colab Notebooks/load/decoder/logs/tests/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# tensorboardCallbackEncoder = TensorBoard(log_dir = logdirEnc)\n","# tensorboardCallbackDecoder = TensorBoard(log_dir = logdirDec)\n","\n","### Create Encoder model\n","\n","# Model layers\n","num_features_encoder = 1 + (num_months + num_days + num_hours)\n","inputEncoder = Input(batch_shape=(batchSize, M, num_features_encoder))\n","encoderLSTM = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful, activation = \"relu\")\n","encoderLSTM2 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","#encoderLSTM3 = LSTM(numNeuron, return_sequences=True, return_state=True, stateful=stateful,activation = \"relu\")\n","encoderDense = TimeDistributed(Dense(1))\n","\n","\n","# Model connections\n","encoderLSTMOutputs  = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","encoderLSTM2Outputs, _, _      = encoderLSTM2(encoderLSTMOutputs)\n","#encoderLSTM3Outputs, _, _= encoderLSTM2(encoderLSTM2Outputs)\n","encoderOutputs           = encoderDense(encoderLSTM2Outputs)\n","\n","\n","encoderModel = Model(inputs=inputEncoder, outputs=encoderOutputs)\n","encoderModel.compile(optimizer='adam', loss='mse')\n","\n","# encoderFilepath=\"/content/drive/My Drive/Colab Notebooks/load/models/encoder-weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n","# encoderCheckpoint = ModelCheckpoint(encoderFilepath, monitor='val_loss', verbose=1, save_best_only=False,save_weights_only=False, mode='min')\n","\n","# print('Training encoder...'); encoderModel.fit(XtrainEncoder, YtrainEncoder,epochs=encoderEpoch, shuffle=False, validation_data=(XtestEncoder, YtestEncoder),callbacks=[encoderCheckpoint,tensorboardCallbackEncoder])\n","encoderModel = load_model('/content/drive/My Drive/Colab Notebooks/encoder_load_e31.hdf5')\n","\n","\n","### Create merged (Encoder + Decoder) model\n","\n","# Model layers\n","num_features_decoder   = (num_months + num_days + num_hours)\n","inputDecoder           = Input(batch_shape=(batchSize, T-M, num_features_decoder))\n","decoderBroadcasted     = Lambda(lambda x: broadcast_to(x, [batchSize, T-M, numNeuron]))\n","decoderConcatenated    = Lambda(lambda x: concat(x, axis=2))\n","decoderAllZeros        = Lambda(lambda x: tf.zeros(shape =(batchSize, numNeuron)))\n","decoderLSTM            = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","decoderLSTM2           = LSTM(numNeuron, return_sequences =True,stateful=stateful,dropout = 0.5,activation=\"relu\")\n","#decoderLSTM3           = LSTM(numNeuron, return_sequences =True,stateful=stateful,activation=\"relu\")\n","decoderDense           = TimeDistributed(Dense(1))\n","layerV                 = Dense(numNeuron)\n","layerVActivation       = Activation('relu')\n","layerV_prime           = Dense(numNeuron)\n","layerV_primeActivation = Activation('relu')\n","\n","# Model connections\n","encoderLSTMOutputs, _, encoder_h = encoderLSTM(inputEncoder) #encoders_c is already equal to encoderLSTM\n","\n","\n","# Map encoder hidden state final to some vector(simple layer)\n","c = layerVActivation(layerV(encoder_h))\n","# Remap the c vector to h_prime\n","h_prime = layerV_primeActivation(layerV_prime(c))\n","\n","# Use h_prime as input to other layers\n","c_broadcasted              = decoderBroadcasted(c)\n","decoderConcatenatedOutputs = decoderConcatenated([inputDecoder, c_broadcasted])\n","\n","# decoderStateZeros : just zeros to feed to the internal state\n","decoderStateZeros  = decoderAllZeros(encoder_h) # dummy argument\n","decoderLSTMOutputs = decoderLSTM(decoderConcatenatedOutputs, initial_state=[decoderStateZeros, h_prime])\n","decoderLSTMOutputs2     = decoderLSTM2(decoderLSTMOutputs)\n","#decoderLSTMOutputs3     = decoderLSTM3(decoderLSTMOutputs2)\n","decoderOutputs     = decoderDense(decoderLSTMOutputs2)\n","\n","\n","mergedModel = Model(inputs=[inputEncoder, inputDecoder], outputs=[decoderOutputs])\n","mergedModel.compile(optimizer='adam', loss='mse')\n","print('Training merged(encoder + decoder) model...');\n","\n","outputs = list()\n","\n","MSEs = list()  \n","Y = np.reshape(YtestDecoder,(384,24))\n","mypath = 'drive/My Drive/Colab Notebooks/load_encoder_e31/models/'\n","modelList = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","# losses = [float(modelList[i][-9:-5]) for i in range(len(modelList))]\n","# losses = pd.DataFrame(losses,columns=['mse'])\n","# best = list(losses.sort_values(by=['mse']).index[0:30])\n","for i in range(len(modelList)):\n","  mod = str(mypath + modelList[i])\n","  mergedModel = load_model(mod)\n","  lstmOuts = mergedModel.predict([XtestEncoder, XtestDecoder])\n","  lstmOuts = np.reshape(lstmOuts,(lstmOuts.shape[0],lstmOuts.shape[1]))\n","  \n","  e = mean_squared_error(Y[383], lstmOuts[383])\n","  print('epoch ' + str(i) + ': ',e)\n","  MSEs.append(e)\n","\n","    \n","\n","#np.savetxt('pv_forecasts.txt', forecasts,delimiter=',')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training merged(encoder + decoder) model...\n","epoch 0:  0.10467874044809904\n","epoch 1:  0.10112596166952374\n","epoch 2:  0.11784212822555158\n","epoch 3:  0.1015719222055224\n","epoch 4:  0.10621091395238376\n","epoch 5:  0.11772072235405845\n","epoch 6:  0.13293850888003997\n","epoch 7:  0.13189223309580347\n","epoch 8:  0.11358428743143467\n","epoch 9:  0.08226117662382378\n","epoch 10:  0.13985049914032077\n","epoch 11:  0.14165580250722026\n","epoch 12:  0.12765024949081058\n","epoch 13:  0.127611663903006\n","epoch 14:  0.14529340865305485\n","epoch 15:  0.12266158304554214\n","epoch 16:  0.17156123007753343\n","epoch 17:  0.18232842117102563\n","epoch 18:  0.2165286736222031\n","epoch 19:  0.13522620823628362\n","epoch 20:  0.20055130042187885\n","epoch 21:  0.18562296880282356\n","epoch 22:  0.3661199011685519\n","epoch 23:  0.3066160275142061\n","epoch 24:  0.2636105299340961\n","epoch 25:  0.31446538672954016\n","epoch 26:  0.27044288950565226\n","epoch 27:  0.22968294239885342\n","epoch 28:  0.2555471259578157\n","epoch 29:  0.28822726660745007\n","epoch 30:  0.30203351005962775\n","epoch 31:  0.3640634336058839\n","epoch 32:  0.5273536258744413\n","epoch 33:  0.3033615386431025\n","epoch 34:  0.12579721681128506\n","epoch 35:  0.32394513135409425\n","epoch 36:  0.2704172840940502\n","epoch 37:  0.2948943828728889\n","epoch 38:  0.35501601346019857\n","epoch 39:  0.36895673063441475\n","epoch 40:  0.31541091904696095\n","epoch 41:  0.26429232227104227\n","epoch 42:  0.39741737093487456\n","epoch 43:  0.2854983787591517\n","epoch 44:  0.252350972441241\n","epoch 45:  0.3310794380700272\n","epoch 46:  0.18843276667887185\n","epoch 47:  0.2059898732247376\n","epoch 48:  0.179137252504769\n","epoch 49:  0.17686115715987927\n","epoch 50:  0.16129867943591972\n","epoch 51:  0.17592685197042135\n","epoch 52:  0.21875736975925877\n","epoch 53:  0.18408515276393758\n","epoch 54:  0.1748062696698666\n","epoch 55:  0.1936679657357807\n","epoch 56:  0.1872825292585433\n","epoch 57:  0.16184401779284008\n","epoch 58:  0.20136793126126384\n","epoch 59:  0.21030831944177564\n","epoch 60:  0.1949939936443437\n","epoch 61:  0.1791225468585899\n","epoch 62:  0.15175964688124724\n","epoch 63:  0.16705701434428213\n","epoch 64:  0.17944193286278645\n","epoch 65:  0.1971899701704758\n","epoch 66:  0.2163704605825761\n","epoch 67:  0.17350561042485713\n","epoch 68:  0.19158981041486675\n","epoch 69:  0.15354502870525502\n","epoch 70:  0.1375470483706285\n","epoch 71:  0.18515241802383478\n","epoch 72:  0.20570068725287052\n","epoch 73:  0.16905509111509276\n","epoch 74:  0.1872984960676296\n","epoch 75:  0.1616715018692011\n","epoch 76:  0.1652835441459171\n","epoch 77:  0.20346359192809396\n","epoch 78:  0.16141890969924785\n","epoch 79:  0.1481566298829686\n","epoch 80:  0.20599733196487105\n","epoch 81:  0.18666187615218757\n","epoch 82:  0.14653772498911732\n","epoch 83:  0.16371109370550194\n","epoch 84:  0.21696846122678215\n","epoch 85:  0.2274053385500232\n","epoch 86:  0.21666733046978445\n","epoch 87:  0.22044136975966924\n","epoch 88:  0.2144861569522886\n","epoch 89:  0.15036394100545566\n","epoch 90:  0.19661669476620305\n","epoch 91:  0.1308425135573202\n","epoch 92:  0.22491062667022357\n","epoch 93:  0.1608557972731973\n","epoch 94:  0.19867376220640184\n","epoch 95:  0.23802146864033577\n","epoch 96:  0.27009575835387306\n","epoch 97:  0.21680071275313498\n","epoch 98:  0.1812280739300168\n","epoch 99:  0.19228960298511052\n","epoch 100:  0.1832386324040535\n","epoch 101:  0.18947619281219039\n","epoch 102:  0.22891258274536996\n","epoch 103:  0.22391342879788945\n","epoch 104:  0.18158863107917766\n","epoch 105:  0.1887402733050325\n","epoch 106:  0.16610574786030183\n","epoch 107:  0.12934179625594336\n","epoch 108:  0.18830134596096473\n","epoch 109:  0.223617060927887\n","epoch 110:  0.19381848720448133\n","epoch 111:  0.2262976874164259\n","epoch 112:  0.19912183879209064\n","epoch 113:  0.19070126634940365\n","epoch 114:  0.1882170784426874\n","epoch 115:  0.22115735625198998\n","epoch 116:  0.20456767005788792\n","epoch 117:  0.22853172452008064\n","epoch 118:  0.21419662000470932\n","epoch 119:  0.2260747313571778\n","epoch 120:  0.2179052074942641\n","epoch 121:  0.1866220838869239\n","epoch 122:  0.18049298233637034\n","epoch 123:  0.18521077568516078\n","epoch 124:  0.2139927337952806\n","epoch 125:  0.2094357194387527\n","epoch 126:  0.2111334269970603\n","epoch 127:  0.20298234702313075\n","epoch 128:  0.19390896268991398\n","epoch 129:  0.19255572272323396\n","epoch 130:  0.1867910269813786\n","epoch 131:  0.19006936940440858\n","epoch 132:  0.16457313062613488\n","epoch 133:  0.19726571516684774\n","epoch 134:  0.22180669333802486\n","epoch 135:  0.21762633409646903\n","epoch 136:  0.1857367935528147\n","epoch 137:  0.1674764456169904\n","epoch 138:  0.20029814613626842\n","epoch 139:  0.20388050129418808\n","epoch 140:  0.22011091039688976\n","epoch 141:  0.23707173041982374\n","epoch 142:  0.23834893424587192\n","epoch 143:  0.22954641919454197\n","epoch 144:  0.22276364291527564\n","epoch 145:  0.22006728528319722\n","epoch 146:  0.18710246306805067\n","epoch 147:  0.1997537622102846\n","epoch 148:  0.2039993847525802\n","epoch 149:  0.215153805647419\n","epoch 150:  0.18418449583527333\n","epoch 151:  0.20924100877322638\n","epoch 152:  0.2064969958447627\n","epoch 153:  0.18539284229885192\n","epoch 154:  0.21194636600798\n","epoch 155:  0.2108441521260819\n","epoch 156:  0.23105196577808218\n","epoch 157:  0.24260503079872406\n","epoch 158:  0.18750756824313697\n","epoch 159:  0.13192737375695773\n","epoch 160:  0.19354977466911705\n","epoch 161:  0.18931319110795752\n","epoch 162:  0.21019196335425153\n","epoch 163:  0.20200628183938954\n","epoch 164:  0.18294341904914738\n","epoch 165:  0.2179324931709196\n","epoch 166:  0.19577774158826453\n","epoch 167:  0.17892001141898028\n","epoch 168:  0.18207261309748515\n","epoch 169:  0.1937419623543585\n","epoch 170:  0.19777466946381939\n","epoch 171:  0.21124481097205816\n","epoch 172:  0.22777023761407691\n","epoch 173:  0.2101855953972137\n","epoch 174:  0.18400672229182605\n","epoch 175:  0.14752679307104188\n","epoch 176:  0.2026130484571611\n","epoch 177:  0.22212288367952593\n","epoch 178:  0.210812481142182\n","epoch 179:  0.22001795979061164\n","epoch 180:  0.216342129228159\n","epoch 181:  0.21913396479292732\n","epoch 182:  0.18491719929830863\n","epoch 183:  0.1992788059088534\n","epoch 184:  0.2021173533176789\n","epoch 185:  0.18958766037246547\n","epoch 186:  0.1781232322934562\n","epoch 187:  0.17837489886549784\n","epoch 188:  0.18724031862003157\n","epoch 189:  0.18900165065904861\n","epoch 190:  0.18459328082114226\n","epoch 191:  0.2032514730227142\n","epoch 192:  0.18877615799600264\n","epoch 193:  0.20542113940250598\n","epoch 194:  0.2089780171652745\n","epoch 195:  0.23964875208315697\n","epoch 196:  0.22637292308934373\n","epoch 197:  0.23001990515913642\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lZ4PAvybED-u","colab_type":"code","outputId":"09ae8362-7c24-4ff4-fe27-d710fa3a493f","executionInfo":{"status":"ok","timestamp":1581069410778,"user_tz":-60,"elapsed":904,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(range(25,73))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(384, 24)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"ocXeWApCCZ-C","colab_type":"code","colab":{}},"source":["from os import listdir\n","from os.path import isfile, join\n","import pandas as pd\n","mypath = 'drive/My Drive/Colab Notebooks/load_encoder_e31/models/'\n","modelList = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n","losses = [float(modelList[i][-9:-5]) for i in range(len(modelList))]\n","losses = pd.DataFrame(losses,columns=['mse'])\n","best = list(losses.sort_values(by=['mse']).index[0:30])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-icS7nGsFhoq","colab_type":"code","outputId":"4a4a4e6e-12ef-4294-f62b-b9a4248e346a","executionInfo":{"status":"ok","timestamp":1580986561422,"user_tz":-60,"elapsed":990,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":979}},"source":["MSE = pd.DataFrame(MSEs, columns=['mse'],index = [list(losses.sort_values(by=['mse']).index[0:30])])\n","MSE.sort_values(by=['mse'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.079869</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.082763</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.083405</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.090285</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.091217</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.094732</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.094818</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.095690</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.096684</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.096790</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.097079</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.097992</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.098444</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.103818</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.105355</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.108958</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.110049</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>0.114108</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.115637</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>0.118957</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>0.120283</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>0.120659</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>0.121581</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0.121657</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.123230</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>0.123549</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0.123810</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>0.124115</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.124373</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>0.133918</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         mse\n","0   0.079869\n","1   0.082763\n","8   0.083405\n","3   0.090285\n","2   0.091217\n","9   0.094732\n","4   0.094818\n","12  0.095690\n","6   0.096684\n","5   0.096790\n","14  0.097079\n","19  0.097992\n","13  0.098444\n","17  0.103818\n","15  0.105355\n","16  0.108958\n","10  0.110049\n","51  0.114108\n","18  0.115637\n","98  0.118957\n","94  0.120283\n","69  0.120659\n","92  0.121581\n","48  0.121657\n","21  0.123230\n","52  0.123549\n","36  0.123810\n","70  0.124115\n","24  0.124373\n","71  0.133918"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"2sE_kC_NG9s1","colab_type":"code","outputId":"87c1e304-0812-42f8-b943-abd0c30ca123","executionInfo":{"status":"ok","timestamp":1580986002849,"user_tz":-60,"elapsed":1104,"user":{"displayName":"Andreas Meyer","photoUrl":"","userId":"11293427509822006353"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["best"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0,\n"," 1,\n"," 8,\n"," 2,\n"," 3,\n"," 4,\n"," 9,\n"," 19,\n"," 17,\n"," 14,\n"," 13,\n"," 12,\n"," 6,\n"," 5,\n"," 10,\n"," 15,\n"," 16,\n"," 51,\n"," 48,\n"," 94,\n"," 70,\n"," 69,\n"," 52,\n"," 92,\n"," 36,\n"," 98,\n"," 21,\n"," 18,\n"," 24,\n"," 71]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"oVSxwWzhF4pV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}